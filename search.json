[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "How to Learn to Code ‚Äì Python for Data Science",
    "section": "",
    "text": "Welcome üëã\nThis site hosts the UNC How to Learn to Code Python curriculum.\nRead lessons here, run small code snippets live in your browser, or launch the full notebook in Google Colab with one click.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>How to Learn to Code ‚Äì Python Class</span>"
    ]
  },
  {
    "objectID": "index.html#lessons",
    "href": "index.html#lessons",
    "title": "How to Learn to Code ‚Äì Python for Data Science",
    "section": "Lessons",
    "text": "Lessons\n\n\n\n#\nTopic\nWebsite page\nOpen in Colab\n\n\n\n\n0\nIntroduction to Programming & Python\nLesson 0\nhttps://colab.research.google.com/github/How-to-Learn-to-Code/python-class/blob/master/Lesson_0_Introduction/Lesson_0_Student_Version.ipynb\n\n\n1\nOperators and Data Types\nLesson 1\nhttps://colab.research.google.com/github/How-to-Learn-to-Code/python-class/blob/master/Lesson_1_Basics/Lesson_1_Student_Version.ipynb\n\n\n2\nIterables, Modules & Libraries\nLesson 2\nhttps://colab.research.google.com/github/How-to-Learn-to-Code/python-class/blob/master/Lesson_2_Control_Structs/Lesson_2_Student_Version.ipynb\n\n\n3\nAbstraction & Functions\nLesson 3\nhttps://colab.research.google.com/github/How-to-Learn-to-Code/python-class/blob/master/Lesson_3_Abstraction_Functions/Lesson_3_Student_Version.ipynb\n\n\n4\nFile I/O & Pandas I\nLesson 4\nhttps://colab.research.google.com/github/How-to-Learn-to-Code/python-class/blob/master/Lesson_4_FileIO/Lesson_4_Student_Version.ipynb\n\n\n5\nPandas II\nLesson 5\nhttps://colab.research.google.com/github/How-to-Learn-to-Code/python-class/blob/master/Lesson_5_Pandas_DataFrame/Lesson_5_Student_Version.ipynb\n\n\n6\nNumPy\nLesson 6\nhttps://colab.research.google.com/github/How-to-Learn-to-Code/python-class/blob/master/Lesson_6_NumPy/Lesson_6_Student_Version.ipynb\n\n\n7\nPlotting\nLesson 7\nhttps://colab.research.google.com/github/How-to-Learn-to-Code/python-class/blob/master/Lesson_7_Plotting/Lesson_7_Student_Version.ipynb\n\n\n8-a\nIntro to scikit-learn\nLesson 8a\nhttps://colab.research.google.com/github/How-to-Learn-to-Code/python-class/blob/master/Lesson_8a_Scikit_Learn/Lesson_8a_Student_Version.ipynb\n\n\n8-b\nThe Python Ecosystem\nLesson 8b\nhttps://colab.research.google.com/github/How-to-Learn-to-Code/python-class/blob/master/Lesson_8b_Python_Ecosystem/Lesson_8b_Student_Version.ipynb\n\n\n8-c\nScanpy for single-cell\nLesson 8c\nhttps://colab.research.google.com/github/How-to-Learn-to-Code/python-class/blob/master/Lesson_8c_scanpy/Lesson_8c_Student_Version.ipynb\n\n\n\n(Rows link to the rendered lesson pages; the right-hand URL opens the same notebook in Colab.)",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>How to Learn to Code ‚Äì Python Class</span>"
    ]
  },
  {
    "objectID": "index.html#try-an-interactive-pyodide-cell",
    "href": "index.html#try-an-interactive-pyodide-cell",
    "title": "How to Learn to Code ‚Äì Python for Data Science",
    "section": "Try an interactive Pyodide cell",
    "text": "Try an interactive Pyodide cell\nBelow is a small Python snippet running in your browser via Pyodide. Click Run Code to execute it:",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>How to Learn to Code ‚Äì Python Class</span>"
    ]
  },
  {
    "objectID": "Lesson_0_Introduction/Lesson_0_Student_Version.html",
    "href": "Lesson_0_Introduction/Lesson_0_Student_Version.html",
    "title": "Lesson 0 - Basic Introduction to Programming in Python",
    "section": "",
    "text": "Open In Colab\n\n\n\nLearning objectives\nStudents will gain an introduction to programming in python, working in interactive notebooks, and learning how to leverage outside resources for coding help. Moreoever students will gain a basic understanding of variables, data types, and working with simple expressions for comparison and computation.\n\nIntroduction to Programming\nIntroduction to Jupyter and Google Colab\nRigor and Reproducibility in your code\n\n\nBasic Introduction to Programming in Python \nPython is a general purpose programming language that is widely used in scientific computing, image analysis, machine learning etc. It allows you to specify a set of instructions, written as a script or a program, to execute some task of interest.\n\nHave any of you used a script before? What did you do to run it? What was the purpose of the script? Did it aid in reproducibility? Was it quicker to run or modify than calculating things individually by hand?\n\nThe purpose of this series of python workshops is not to give you an extensive in depth overview of everything you can do in python. However, we do aim to give you all the skills and terminology necessary to learn how to learn to code in python. Google, stack overflow, and github issues) are invaluable tools you can use to your advantage for (1) gaining coding help and (2) learning how to write code from reading code.\n\n\nIntroduction to Jupyter Notebooks and Google Colab \nWe will be using the online server Google Collab for learning python. This server creates an isolated ‚Äúenvironment‚Äù for you to work in. Using your Google account (or you will have to create one) open up Google Collab in your web browser. Start a ‚Äúnew notebook‚Äù to create a new python file.\nThere are many types of files that you can use for Python coding. The most important for you now are : * .py : This is a python script file. All things in this file have to be python code. You would use this file for a complete analysis. * .ipynb : This is a Jupyter Notebook file. Jupyter is a user interface to seemlessly code in python. This worksheet was made in a Jupyter Notebook!\n‚ÄúThe Jupyter Notebook is an incredibly powerful tool for interactively developing and presenting data science projects. A notebook integrates code and its output into a single document that combines visualizations, narrative text, mathematical equations, and other rich media. The intuitive workflow promotes iterative and rapid development, making notebooks an increasingly popular choice at the heart of contemporary data science, analysis, and increasingly science at large.‚Äù - dataquest\nAll of our lessons will be presented in Jupyter notebooks due to their interactive nature (.ipynb file extension). They consist of two main attributes, a kernel and cells. * A kernel interprets and executes the code. Here we are using the kernel for python; however, you can specify a kernel for another language like R. * A cell is a container for either text or code to be executed.\nTo run the python code in a cell, you just hit shift + enter. Try it with the code below.\n\nprint('hello world')\n\n\n\nRigor and Reproducibility In Your Code \nSimilar to an author writing, a programmer will adopt a style of coding. This can be as small as how many new lines you have between code lines, to the way you comment your code, and how you name your variables. Matching a standard style of code will help make your code more readable and more reproducible. If you continue your coding journey, we recommend the PEPS Style Guide for Python. Go ahead and look through this resource, but focus on the Commenting, Code Lay-out, and Naming Convention sections.\n\nCommenting Your Code\nOne of the most important things you can do to learn and to help other programmers read your code, is using meaningfull comments. Comments will begin with a # sign within a code chunk. You can have an inline comment, to provide information for a speicifc like of code, or a block of comments to describe a section. You want these comments to be concise but informative.\nAt the begining of every file, use a block of comments to outline the python file youre making. This will also be good practice for documentation for any public code you publish‚Ä¶ Like this:\n\n# File name     : lesson_0_Student_version\n# Goal          : This script is a practice for the How to Learn to Code class. It is introducing reproducible code using Jupyter notebook and coding styles. \n# Input         : NA\n# Output        : NA\n# Libraries     : NA\n# Required      : Google account\n\nExamples of input would be the datasets you need, other files that are used in this script, ect. You also may have figures as outputs, or processed data. List all of these at the top of your code, so that you can find the information quickly when you return. If youre using a notebook file, like you will be in this class, feel free to add this in a ‚ÄúMarkdown‚Äù chunk instead.\nYour inline comments should be added to describe an important line of code. Look at the formatting below:\n\ni = 2\n\n# Good commenting style :\n\nif i & (i-1) == 0:  # True if i is 0 or a power of 2.\n\n# Bad commenting style : \n\nif i & (i-1) == 0: #checking i\n\n\n\nNaming Conventions\nWhen you are naming somehting in your code, like a variable here, it is important to follow a few rules: * always start the name with a letter (no numbers or special characters) * never start a name with an underscore (this is reserved for official Python variables nad functions) * avoid long variable names * use a specific naming style (we will use ‚Äúsnake case‚Äù which is all lowercase, and has an underscore seperating words)\n\n# Good naming style : \n\napples = ['granny_smith','machintosh','golden_delicious']\ngranny_smith = 'green'\nmachintosh = 'red'\ngolden_delicious = 'yellow'\n\n# Bad naming style :\n\nlist_ofApples=['gs','MACIN','_golden_']\ngs= 'green'\nMACIN =\"Red'\n\n\n_Golden='y'\n\nFinally, when naming your files, find a format that works well for you, but follow these few rules : * if you start your files with a number, and you expect to have more than 10 (or more than 100) add a zero to the begining of the file name. Computers will treat the number in the name as part of the alphabet, so if you don‚Äôt do this, the computer orders them incorrectly * WRONG : 1_facs_data.fcs, 10_facs_data.fcs, 11_facs_data.fcs ‚Ä¶ 2_facs data.fcs * RIGHT : 01_facs_data.fcs, 02_facs_data.fcs, ‚Ä¶.. 10_facs_data.fcs * RIGHT : 001_facs_data.fcs, 02_facs_data.fcs, ‚Ä¶.. 010_facs_data.fcs ‚Ä¶ 100_facs_data.fcs\n\nnever have a space in your file name\n\nWRONG : facs data.fcs\nRIGHT : facs_data.fcs\n\nFor a project, always name your files similar to eachother\n\nWRONG : 01_facs_data.fcs, facsdata2.fsc, 3_facs_data.fsc, mouse4_facs_data_.fsc",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Lesson 0 - Basic Introduction to Programming in Python</span>"
    ]
  },
  {
    "objectID": "Lesson_1_Basics/Lesson_1_Student_Version.html",
    "href": "Lesson_1_Basics/Lesson_1_Student_Version.html",
    "title": "Lesson 1 ‚Äì Basic Introduction to Programming in Python",
    "section": "",
    "text": "Open In Colab\n\n\n\nLesson 1 - Basic Introduction to Programming in Python : Operators and Data Types\ntitle: ‚ÄúLesson 1 ‚Äì Basic Introduction to Programming in Python: Operators and Data Types‚Äù jupyter: python3 format: live-html\n\nLearning objectives\nStudents will gain an introduction to programming in python, working in interactive notebooks, and learning how to leverage outside resources for coding help. Moreoever students will gain a basic understanding of variables, data types, and working with simple expressions for comparison and computation.\n\nBasic math operators: +, -, *, /, **\nUse of the assignment operator (=)\nData types: integer, float, string, boolean\nBasic functions: print(), type(), int(), float(), str(), bool()\nComparison operators: &gt;, &lt;, ==, &gt;=, &lt;=\nIn Class Exercises\n\n\nBasic Introduction to Programming in Python \nPython is a general purpose programming language that is widely used in scientific computing, image analysis, machine learning etc. It allows you to specify a set of instructions, written as a script or a program, to execute some task of interest.\n\nHave any of you used a script before? What did you do to run it? What was the purpose of the script? Did it aid in reproducibility? Was it quicker to run or modify than calculating things individually by hand?\n\nThe purpose of this series of python workshops is not to give you an extensive in depth overview of everything you can do in python. However, we do aim to give you all the skills and terminology necessary to learn how to learn to code in python. Google, stack overflow, and github issues) are invaluable tools you can use to your advantage for (1) gaining coding help and (2) learning how to write code from reading code.\n\n\nIntroduction to Jupyter Notebooks and Google Colab \n‚ÄúThe Jupyter Notebook is an incredibly powerful tool for interactively developing and presenting data science projects. A notebook integrates code and its output into a single document that combines visualizations, narrative text, mathematical equations, and other rich media. The intuitive workflow promotes iterative and rapid development, making notebooks an increasingly popular choice at the heart of contemporary data science, analysis, and increasingly science at large.‚Äù - dataquest\nAll of our lessons will be presented in Jupyter notebooks due to their interactive nature (.ipynb file extension). They consist of two main attributes, a kernel and cells. * A kernel interprets and executes the code. Here we are using the kernel for python; however, you can specify a kernel for another language like R. * A cell is a container for either text or code to be executed.\nTo run the python code in a cell, you just hit shift + enter. Try it with the code below.\n\nprint('hello world')\n\n\n\nBasic math operators: +, -, *, /, ** \nThe simplest way we can use python is to use is as a calculator! You can use python to do addition, subtraction, division, multiplication, and exponentiation. Let‚Äôs walk through some simple math calculations.\n\\[ 2 + 3 \\]\n\\[ \\frac{6}{2} * 20 - 100 \\]\n\\[ 2^{8} \\]\n\nWas the above result what you expected? If you used ^ then your result was 10; however, if you used ** then your result was 256. This demonstrates why it‚Äôs important to check the answers you get from your code to make sure the results seem reasonable. You might not always get an error when your code is incorrect :)\n\n\n\nUse of the assignment operator (=) \nWhat if you want to store some value (like the result of the above calculation) and use it for another task later? Here, you can assign it to a variable with the assignment operator =.\nFor example, let‚Äôs assign the value 22 to variable x as,\n\nx = 22\n\nNotice that the code above did not return any result. That‚Äôs because it is now stored in the variable called x!\n\nx\n\nTry assigning the values 1, 2, and 3 to the variables x, y, and z respectively.\nIf you‚Äôd like to see what the values of the variables were, you can use the print() function. Here, the input of the function (what goes inside the parantheses) is whatever you‚Äôd like to display to the terminal.\n\nprint(x)\nprint(y)\nprint(z)\n\nOnce you have variables assigned, you can use them to do basic operations. For example, try calculating the following. Be sure to use the assingment operator when necessary and print your results to check that they make sense.\n\nAssign the variable a as the result of \\(4 + 5*2\\)\nCompute \\(5 + a\\)\nAssign the variable b as the result of \\(\\frac{a}{2.5}\\)\n\nOf note, you can name a variable anything you‚Äôd like (although it‚Äôs best to be descriptive for you and others who will end up reading your code). However, a variable name must start with a letter and not a number. For fun, try assigning the value 3 as the variable 1variable and see what the result is.\n\n\nData types: integer, float, string, boolean \nProgramming languages often include different data types that can be used for different tasks (e.g.¬†arithmetic, comparison, debugging etc.). Here we‚Äôll discuss four of the most common types: 1. integer: whole number 2. float: decimal number 3. string: text 4. boolean: True or False\nIntegers \nIntegers are positive or negative whole numbers with no decimals. We‚Äôve already been using them it today‚Äôs lesson. For example, print the variables x, y, z, and a.\nFloats \nFloats, short for floating point real values, are positive or negative numbers with decimals. As with integers, you can use them to do arithmetic or assign them to variables. Try defining and printing the following expressions.\n\nf1 assigned to value 4.5\nf2 assigned to value -3.0\nf1 + f2\n\nStrings \nStrings refer to text (ie they‚Äôre a series of characters) and can be (1) assigned to variables for data manipulation or (2) can be used in print statements for debugging or monitoring tasks. You define them by placing the text within single or double quotation marks as,\n\noligo1 = 'GCGCTCAAT'\noligo2 = 'TACTAGGCA'\nprint(oligo1, oligo2)\nprint('string variable 1 is', oligo1)\n\nBooleans \nBooleans are either True or False. Note the capitilization of the first letter. They are often used for comparisons (e.g.¬†the output of \\(1 &gt; 2\\) would result in False). Additionally, it‚Äôs important to note that True behaves as 1 and False behaves as 0 in arithmetic operations.\n\nt = True\nf = False\n\nprint(2*t)\nprint(2*f)\n\n\n\nComparison operators: &gt;, &lt;, ==, &gt;=, &lt;= \nComparison operators (&gt;, &lt;, ==, &gt;=, &lt;=) can be used to compare two or more variables. The output of the comparison is a Boolean (ie True or False). Therefore, we often use comparison operators to set up condition statements (e.g.¬†if this comparison statement is True, then do this task. If it is False, then do something else). You‚Äôll learn more about condition statements and control in Lesson 2.\nLet‚Äôs set up a few comparisons and use print statements to see the results.\n\\[ 2 &gt; 1.5 \\] \\[ 4 &lt;= 2 \\] \\[ 5 == 5.0 \\] \\[ 6 &gt;= 6 \\] \\[ 7.4 &lt; 3 \\]\n\n\nBasic functions: type(), int(), float(), str(), bool() \nWe‚Äôve already started using one basic function, print(). Each function has a goal and does a task. How it works is it takes whatever is specified within the parantheses (the arguments), performs the task, and then outputs the results. Functions are extraordinarily useful for reproducible code and you‚Äôll learn more about them in Lesson 3.\nFor now, let‚Äôs explore some other basic functions. The function type() returns the data type of the argument you give it. See the example below.\n\ntype(2.1)\n\n\ntype('hello world')\n\n\ntype(1)\n\n\ntype(True)\n\nThere are other useful functions that let you specify the data type or coerce the data type into another. int(), float(), bool(), and str() let you specify data as an integer, float, boolean, or string. Let‚Äôs take a look at how this works for the int() and str() functions.\n\nint(2.1)\n\n\nstr(20)\n\n\n\nIn Class Exercises \nThe following exercises will help you understand the concepts taught in this lesson better.\n\nLike in algebra, parentheses can be used to specify the order of operations. What would you expect to be the result of the following expressions? (Try to predict the answer before checking your answers in python.) \\[1 + 3*3\\] \\[(1 + 3)*3\\] \\[(4*4) + 5\\] \\[1 &gt; 3\\] \\[\"1\" &gt; \"3\"\\] \\[14 &gt;= 2*7\\] \\[0 == \\mathrm{False}\\] \\[\\mathrm{True} == 1^{0}\\] \\[\\mathrm{int}(\\mathrm{True}) == \\mathrm{str}(1^{0})\\] \\[\\mathrm{float(\"one\")}\\]\nCreate two new variables, comp_oligo1 and comp_oligo2, that are the complementary DNA sequences of oligo1 and oligo2 (hint: A &lt;-&gt; T and G &lt;-&gt; C)\nCompute the melting temperature of oligo1 and oligo2 using the following equation: \\[\\mathrm{T_{m}} = 2(\\mathrm{A} + \\mathrm{T}) + 4(\\mathrm{G} + \\mathrm{C})\\] where A, T, G, and C refer to the number of A, T, G, C nucleotides in the primer.\n\n\n#question 1\n\n\n#question 2\n\n\n#question 3",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Lesson 1 ‚Äì Basic Introduction to Programming in Python</span>"
    ]
  },
  {
    "objectID": "Lesson_2_Control_Structs/Lesson_2_Control_structs_student.html",
    "href": "Lesson_2_Control_Structs/Lesson_2_Control_structs_student.html",
    "title": "Lesson 2 - Data structures, Control flows, and Python Ecosystems",
    "section": "",
    "text": "Learning objectives:\nStudents will be able to use data structures and control flows to make algorithms.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Lesson 2 - Data structures, Control flows, and Python Ecosystems</span>"
    ]
  },
  {
    "objectID": "Lesson_2_Control_Structs/Lesson_2_Control_structs_student.html#introduction",
    "href": "Lesson_2_Control_Structs/Lesson_2_Control_structs_student.html#introduction",
    "title": "Lesson 2 - Data structures, Control flows, and Python Ecosystems",
    "section": "Introduction",
    "text": "Introduction\nData structures are basically just that - they are structures which can hold some data together. In other words, they are used to store a collection of related data. These are particularly helpful when working with experimental data sets. There are four built-in data structures in Python - list, tuple, dictionary and set.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Lesson 2 - Data structures, Control flows, and Python Ecosystems</span>"
    ]
  },
  {
    "objectID": "Lesson_2_Control_Structs/Lesson_2_Control_structs_student.html#data-structures",
    "href": "Lesson_2_Control_Structs/Lesson_2_Control_structs_student.html#data-structures",
    "title": "Lesson 2 - Data structures, Control flows, and Python Ecosystems",
    "section": "Data structures",
    "text": "Data structures\nThere are four built-in data structures in python: lists,tuples, sets, and dictionary. In this lesson, we will learn about each data structures.\n\nLists\nA list is a data structure that holds an ordered collection of items i.e.¬†you can store a sequence of items in a list. This is easy to imagine if you can think of a shopping list where you have a list of items to buy, except that you probably have each item on a separate line in your shopping list whereas in Python you put commas in between them.\nHere are important properties of lists:\n\nLists are ordered ‚Äì Lists remember the order of items inserted.\nAccessed by index ‚Äì Items in a list can be accessed using an index.\nLists can contain any sort of object ‚Äì It can be numbers, strings, tuples and even other lists.\nLists are changeable (mutable) ‚Äì You can change a list in-place, add new items, and delete or update existing items.\n\nLet‚Äôs create a list containing five different genes. In Python, a list is created by placing elements inside square brackets [], separated by commas.\n\ngene_list = ['EGFR', 'KRAS', 'MYC', 'RB', 'TP53']\nprint(gene_list)\n\n['EGFR', 'KRAS', 'MYC', 'RB', 'TP53']\n\n\n\n\nList operations\nNow let‚Äôs learn what we can do with lists.\n\nAccessing list elements: indexing/slicing\nWe can access list items using index opreator. In python, indices start at 0.\n\n\n\nindexing\n\n\nLet‚Äôs try to access the second gene in our gene_list.\n\ngene_list[1]\n\n'KRAS'\n\n\nPython also has ‚Äònegative indices‚Äô which can be very convinient when we want to access last i-th item. Last item in the list can be accessed with index of -1. Let‚Äôs try to access the second to last item in gene_list.\n\ngene_list[-2]\n\n'RB'\n\n\nWe can also access a range of items in al list using the slicing operator :. Let‚Äôs try to access the first three items in gene_list. Note that the start index is inclusive, but end index is exclusive. We will see later when this can be useful.\n\n# first three items\nprint(gene_list[0:3])\n# beginning index (0) can be ommitteed\nprint(gene_list[:3])\n\n['EGFR', 'KRAS', 'MYC']\n['EGFR', 'KRAS', 'MYC']\n\n\nWhat should we do if we want to access the last three items?\n\n# last three items\n\n\n\nList methods\nNow, we are interested in a new cancer gene, PTEN, and want to add this to our gene list. How should we do this? There are a few ways to do this, and one way is using a list method called, append.\n\ngene_list.append('PTEN')\ngene_list\n\n['EGFR', 'KRAS', 'MYC', 'RB', 'TP53', 'PTEN']\n\n\nOop, we were supposed add BRAF instead of PTEN! What should we do? Since lists are mutable, we can repalce PTEN with BRAF.\n\ngene_list[-1]\n\n'PTEN'\n\n\n\ngene_list[-1] = 'BRAF'\ngene_list\n\n['EGFR', 'KRAS', 'MYC', 'RB', 'TP53', 'BRAF']\n\n\nThere are other useful list methods that we can use to alterate and describe lists.\n\n\nSome useful functions and list methods\n\nlist.append(x): Add an item x to the end of the list.\nlist.remove(x): Remove the first item from the list whose value is equal to x.\nlist.count(x): Return the number of times x appears in the list.\nreversed(l): Reverse the order of items in list l and return the new list.\nsorted(l): Sort the order of values in list l (default is in ascending order) and return the new list.\nlen(l): Return the length of list l\n\n\n\n\nTuples\nTuples are ordered collection of values. Tuples are similar to lists with one major difference. They are are immutable, meaning that we cannot change, add, or remove items after they are created. We an create a tuple by placing comma-seperated values inside ().\n\ngene_tuple = ('EGFR', 'KRAS', 'MYC', 'RB', 'TP53')\n\nSimilarly to lists, we can use indexing and slicing to access items\n\nprint(gene_tuple[2])  # access third item\nprint(gene_tuple[-2:])  # access last two items\n\nMYC\n('RB', 'TP53')\n\n\nCan we replace KRAS with NRAS for tuples?\n\ngene_tuple[1] = 'NRAS'\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-64-d296efa189c8&gt; in &lt;module&gt;\n----&gt; 1 gene_tuple[1] = 'NRAS'\n\nTypeError: 'tuple' object does not support item assignment\n\n\n\nThis raises error because tuples are immutable.\n\n\nSets\nPython set is an unordered collection of unique items. They are commonly used for computing mathematical operations such as union, intersection, difference, and symmetric difference.\n\n\n\nset operations\n\n\nThe important properties of Python sets are as follows:\n\nSets are unordered ‚Äì Items stored in a set aren‚Äôt kept in any particular order.\nSet items are unique ‚Äì Duplicate items are not allowed.\nSets are unindexed ‚Äì You cannot access set items by referring to an index.\nSets are changeable (mutable) ‚Äì They can be changed in place, can grow and shrink on demand.\n\nSets can be created by placing items comma-seperated values inside {}\nWe have upregulated genes in tumor tissues compared to normal tissues from two patients. We would like to know if there is a shared upregulated genes.\n\npatient1 = {'ABCC1', 'BRCA1', 'BRCA2', 'HER2'}\npatient2 = {'BRCA1', 'HER2', 'ERCC1'}\n\n\n# set intersection\nprint(patient1.intersection(patient2))  # use intersection method\nprint(patient1 & patient2)  # use & operator\n\n{'BRCA1', 'HER2'}\n{'BRCA1', 'HER2'}\n\n\n\n\nDictionaries\nA dictionary is like an address-book where you can find the address or contact details of a person by knowing only his/her name i.e.¬†we associate keys (name) with values (details). Note that the key must be unique just like you cannot find out the correct information if you have two persons with the exact same name.\nNote that you can use only immutable objects (like strings) for the keys of a dictionary but you can use either immutable or mutable objects for the values of the dictionary.\nPairs of keys and values are specified in a dictionary by using the notation d = {key1 : value1, key2 : value2 }. Notice that the key-value pairs are separated by a colon and the pairs are separated themselves by commas and all this is enclosed in a pair of curly braces.\nThe following example of a dictionary might be useful if you wanted to keep track of ages of patients in a clinical trial.\n\nagesDict = {'Karen P.' : 53, 'Jessica M.': 47, 'David G.' : 45, 'Susan K.' : 57, 'Eric O.' : 50}\nprint(agesDict)\n\nWe can access a person‚Äôs age (value) using his/her name (key). Let‚Äôs find out Eric O.‚Äôs age.\n\nagesDict['Eric O.']\n\n50\n\n\nA new patient is enrolled into the clinical trial. Her name is Hannah H. and her age is 39. We can add a new item to the dictoary.\n\nagesDict['Hannah H.'] = 39\n\n\nprint(agesDict)\n\n{'Karen P.': 53, 'Jessica M.': 47, 'David G.': 45, 'Susan K.': 57, 'Eric O.': 50, 'Hannah H.': 39}\n\n\n\nData Structures Summary\nNow we have learned the four basic python data structures: list, tuple, set, and dictionary. We have jsut touched the surface of these data structures. To learn more about these data structures and how to use them, please refer to the references!\n\n\n\nExercises Part 1\nThe following exercises will help you better understand data structures.\n\nMake a list containing the following numbers 1, 4, 25, 7, 9, 12, 15, 16, and 21. Name the list num_list\n\n\n# Q1\n\n\nFind the following information about the list you made in Q1: length, minimum, and maximum. You may need to google to find functions that can help you.\n\n\n# Q2\n# length\n\n9\n\n\n\n# minimum\n\n1\n\n\n\n# maximum\n\n25\n\n\n\nMake a dictionary that describes the price of five medications. Name the dictionary med_dict &gt;* Lisinopril: 23.07 &gt;* Gabapentin: 86.27 &gt;* Sildenafil: 169.94 &gt;* Amoxicillin: 17.76 &gt;* Prednisone: 13.81\n\n\n# Q4\n\n\nUse med_dict to calculate how much it will cost if a patien tis treated with Lisinopril and Prednisone.\n\n\n# Q5\n\n36.88",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Lesson 2 - Data structures, Control flows, and Python Ecosystems</span>"
    ]
  },
  {
    "objectID": "Lesson_2_Control_Structs/Lesson_2_Control_structs_student.html#control-flows",
    "href": "Lesson_2_Control_Structs/Lesson_2_Control_structs_student.html#control-flows",
    "title": "Lesson 2 - Data structures, Control flows, and Python Ecosystems",
    "section": "Control flows",
    "text": "Control flows\n\nIf‚Ä¶else statment\nDecision making is required when we want to execute a code only if a certain condition is satisfied.\nThe if‚Ä¶elif‚Ä¶else statement is used in Python for decision making. We can use these statments to execute a block of code only when the condition is true. The if‚Ä¶elif‚Ä¶else statement follows this syntax. Note, elif is abbreviation for else if.\nif condition1:\n    statment1\nelif condition2:\n    statement2\nelse:\n    statement3\n\n\n\nifelse syntax\n\n\nLet‚Äôs think of a dose-finding clinical trial. We first treat three patients with dose x. iIf no patients shows toxic side effects, we increase the dose. If one patient shows toxicity, we treat another three patients to learn more. If more than one patients show toxicity, we stop at that dose. Let‚Äôs make this into python code. You can change the value of n_toxic to see how the script works.\n\nn_toxic = 2\nif n_toxic == 0:\n    print(\"increase dose\")\nelif n_toxic == 1:\n    print(\"treat another three patients\")\nelse:\n    print(\"stop\")\n\nstop\n\n\n\n\nLoops\nThere are two types of loops in python: while loops and for loops. Loops are useful when we want to performe the same task repetitively.\n\nWhile loop\nA while loop is used when you want to perform a task indefinitely, until a particular condition is met. For instance, we want to enroll new patients to a clinical trial until we have 30 patients.\n\nn_patients = 1\nwhile n_patients &lt;= 30:\n    print(\"enrolled patient\", n_patients)\n    n_patients += 1\n\nenrolled patient 1\nenrolled patient 2\nenrolled patient 3\nenrolled patient 4\nenrolled patient 5\nenrolled patient 6\nenrolled patient 7\nenrolled patient 8\nenrolled patient 9\nenrolled patient 10\nenrolled patient 11\nenrolled patient 12\nenrolled patient 13\nenrolled patient 14\nenrolled patient 15\nenrolled patient 16\nenrolled patient 17\nenrolled patient 18\nenrolled patient 19\nenrolled patient 20\nenrolled patient 21\nenrolled patient 22\nenrolled patient 23\nenrolled patient 24\nenrolled patient 25\nenrolled patient 26\nenrolled patient 27\nenrolled patient 28\nenrolled patient 29\nenrolled patient 30\n\n\nWhat do you think will hapen if we use while True?\n\n\n\nfor loop\nFor loops are used for iterating over a sequence of objects i.e.¬†go through each item in a sequence. Iterable of items include lists, tuples, dictonaries, sets, and strings. For loop has the following syntax:\nfor var in iterable:\n    statement\nLet‚Äôs look at an example of a for loop.\n\nfor i in [1, 2, 3, 4, 5]:\n    print(i)\n\n1\n2\n3\n4\n5\n\n\nWe can also use the range function to do the same thing. range function takes three parameters: start(default 0), end, and steps (default 1). Like slicing, the start is inclusive but the end is exclusive.\n\nfor i in range(0, 6):\n    print(i)\n\n0\n1\n2\n3\n4\n5\n\n\n\n# 0 can be omitted.\nfor i in range(6):\n    print(i)\n\n0\n1\n2\n3\n4\n5\n\n\n\n\nExercise Part 2\nBefore going into exercise, we need to learn a handy operation +=.\n\na = 0\na = a + 1\n\nis equivalent to\n\na = 0\na += 1\n\n\nThe most exciting part about control flows is that they can be nested to make more complex algorithms. Let‚Äôs look at the complementary DNA sequences that we discussed in lesson 1. Solve this problem using for and if.\n\n\nCreate two new variables, comp_oligo1 and comp_oligo2, that are the complementary DNA sequences of oligo1 and oligo2 (hint: A &lt;-&gt; T and G &lt;-&gt; C)\n\n\noligo1 = 'GCGCTCAAT'\noligo2 = 'TACTAGGCA'\n\n\n# backbone\ncomp_oligo1 = ''\nfor nuc in oligo1:\n    if nuc == #something:\n        comp_oligo1 += #something\n    ...\n\n\n  File \"&lt;ipython-input-2-3a5720451dff&gt;\", line 3\n    if nuc == #something\n              ^\nSyntaxError: invalid syntax\n\n\n\n\n\nLet‚Äôs go back to the dose-finding clinical trials. We have a list of doses that we want to test. dose_list = [1, 2, 3, 5, 8, 13]. We want to increase the dose until we hit the maximal tolerated dose (MTD). For simplicity, we will increase dose when there is less than two patients out of three patients with toxicity and stop otherwise. The last dose before at least two patients have toxicity is declared MTD. We will look into the future and assume that we know how many patients will have toxicity at each dose tox_list = [0, 0, 1, 1, 2, 2]. Find the MTD using while.\n\n\ndose_list = [1, 2, 3, 5, 8, 13]\ntox_list = [0, 0, 1, 1, 2, 2]\n\n\n\n\n5\n\n\nHopefully, you can see how importing data from and exporting data to csv or other delimiter separated files will be helpful for research. Now it‚Äôs time to practice your skills with built-in data structures and data frames!\n\n\nSources and References\nhttps://www.programiz.com/python-programming/list\nhttps://python.swaroopch.com/data_structures.html\nhttps://docs.python.org/3/tutorial/datastructures.html\nhttps://www.learnbyexample.org/python-tuple/",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Lesson 2 - Data structures, Control flows, and Python Ecosystems</span>"
    ]
  },
  {
    "objectID": "Lesson_3_Abstraction_Functions/functions_student.html",
    "href": "Lesson_3_Abstraction_Functions/functions_student.html",
    "title": "Lesson 3 - Abstraction, Functions, and Scope",
    "section": "",
    "text": "Introduction\nLearning objectives: Students will be able to design, use, and write functions with consistent variable scoping and design choices.\nSpecific skills: * Writing psuedocode * Writing basic functions * Using functions * Using multiple arguments in a function and calling them inside the scope of the function * Writing local and global variables and pass them between scopes * Adding default values to a function parameter * Docstrings * Using assert to test a functions output\nTable of Contents\nAbstraction is a core concept in programming; it‚Äôs the idea of taking something complex and assigning it a simpler and more reusable form. You‚Äôve already learned about a form of abstraction in lessons 1 and 2: variables. When you assign a string, number, list, etc. to a variable you‚Äôre separating the item (the data in the variable) from the accession (the variable name).\nThis same idea can be applied to processes; these are called functions. Functions allow you to execute a series of commands using a single execution call and can be written to process multiple inputs by being called multiple times.\nAbstraction is a fundamentally difficult concept to grasp in programming, so don‚Äôt feel bad if it takes a bit to wrap your head around.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Lesson 3 - Abstraction, Functions, and Scope</span>"
    ]
  },
  {
    "objectID": "Lesson_3_Abstraction_Functions/functions_student.html#motivating-example",
    "href": "Lesson_3_Abstraction_Functions/functions_student.html#motivating-example",
    "title": "Lesson 3 - Abstraction, Functions, and Scope",
    "section": "Motivating Example",
    "text": "Motivating Example\nConsider a common training task: building a calculator that can do the following with two numbers: - Add - Subtract - Multiply - Divide - Calculate an exponent (raise the first number to the power of the second)\nWe choose this because it is straightforward to understand, has a clear requirement for inputs/outputs, and is easily testable. You already learned how to do all of these mathematical operations, so the only novel concept will be related to building and running functions.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Lesson 3 - Abstraction, Functions, and Scope</span>"
    ]
  },
  {
    "objectID": "Lesson_3_Abstraction_Functions/functions_student.html#pseudocode",
    "href": "Lesson_3_Abstraction_Functions/functions_student.html#pseudocode",
    "title": "Lesson 3 - Abstraction, Functions, and Scope",
    "section": "Pseudocode",
    "text": "Pseudocode\nWhen writing functions it‚Äôs important to think about 3 things: - What does my function need to do? or What output should my function return? - What input does it require? - What steps need to happen to go from input to output?\nWriting out these requirements and steps in natural language is an organized and efficient way to understand your function before writing it, which makes the creation and testing easier. This is called pseudocode because it is not code but explains the logic behind a section of code that‚Äôs organized in the same way as the code itself.\nHere‚Äôs how I would write these notes for our calculator example:\n\nInput:\n\nMathematical operation\nNumber 1\nNumber 2\n\nOutput:\n\nResulting value from operation\n\nProcess (pseudocode):\n\nCheck for inputted operation\nCalculate the operation using two inputted values\nReturn resulting value\n\n\n\nFirst example\nWith that, let‚Äôs write our first, and simplest, form of a function: a function to add two numbers.\n\ndef add(num_1, num_2):\n\n    result = num_1 + num_2\n    \n    return result\n\n\n\"\"\"\n#Note: you can also return without an intermediate variable\n\ndef add(num_1, num_2):\n    return num_1 + num_2\n\"\"\"\n\nNotice how the function has three major components: - def add(num_1, num_2): which defines the name of the function and the inputs in the function declaration. - The commands to be run whenever the function is run, we only have one in this case: result = num_1 + num_2. - return result, which tells the function that it‚Äôs done everything it needs to and what value(s) to give to the global scope (more on that later). Whatever value(s) are returned can then be set as a variable when you call the function, as you‚Äôll see in the next block of code. - Note: You can return multiple values by giving a comma separated list of them, e.g.¬†return foo, bar, baz\nAlso notice that if you try to run the code block above nothing happens. This is because all we‚Äôve done so far is define the function, meaning we built the machinery, but we haven‚Äôt actually ran it yet.\nYou‚Äôve probably already run a function without realizing it though using the humble print(). If you‚Äôve printed anything using that, you‚Äôve run a function. print() takes in one or more strings and outputs them to the terminal. Calling a function is usually as simple as calling print, you type the name of the function followed by parentheses containing the inputs.\nLet‚Äôs give this a go with our adding function in a couple different flavors, all of which are functionally identical:\n\n#All are equivalent\n\n#Positional args\nadd_result = add(2, 8)\n\n#Keyword args\nadd_result = add(num_1=2, num_2=8)\n\n#Positional args with variables\nnum_1 = 2\nnum_2 = 8\nadd_result = add(num_1, num_2)\n\n#Keyword args with variables\nnum_1 = 2\nnum_2 = 8\nadd_result = add(num_1=num_1, num_2=num_2)\n\nAs you can see, there are a few different ways you can specify inputs for functions: 1. Positional arguments: assume the inputs are given in the same order as defined in the function definition (first argument for add() is always num_1, second is always num_2). 2. Keyword arguments: explicitly state which argument you are assinging during the function use, non-ordered.\nBoth methods are used in different contexts, generally it comes down to how complex a set of inputs are and your preferred style, just remember that positional args are dependent on the order they‚Äôre in during the definition, so it‚Äôs easier to lead to errors.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Lesson 3 - Abstraction, Functions, and Scope</span>"
    ]
  },
  {
    "objectID": "Lesson_3_Abstraction_Functions/functions_student.html#scope",
    "href": "Lesson_3_Abstraction_Functions/functions_student.html#scope",
    "title": "Lesson 3 - Abstraction, Functions, and Scope",
    "section": "Scope",
    "text": "Scope\nNow that we‚Äôre talking about input variables it‚Äôs important to consider scope. This is the concept that whatever happens in a function stays in a function unless returned, but the opposite is not true.\nTake the demonstrations for example:\n\n#Global scope: accessible to all\nglobal_scope_tester = \"foo\"\nprint(f\"Before function; outside of function scope: {global_scope_tester}\")\n\ndef print_scope():\n    print(f\"Inside function; outside of function scope: {global_scope_tester}\")\n\n    local_scope_tester = \"bar\"\n    print(f\"Inside function; inside of function scope: {local_scope_tester}\")\n\n    return None\n\nprint_scope()\n\nWe can see that global variables are accessible to: - Code outside of functions - Code inside functions without passing them as input - (Warning: this is bad practice and should not be used often; leads to difficult testing and untraceable code)\nYou may notice that there is still a scenario missing: what if you try to print local_scope_tester outside of the function?\nTry to run the following:\n\nprint(f\"Inside function; outside of function scope: {local_scope_tester}\")\n\nAs you may have guessed, this code doesn‚Äôt run. In fact, if you check carefully, you can see that the error type is a NameError, meaning that according to wherever the code is looking there isn‚Äôt a variable named local_scope_tester. That variable is considered local to the function, and unless you return the value you can‚Äôt access it outside of the function.\nHere‚Äôs how to fix that:\n\n#Global scope: accessible to all\nglobal_scope_tester = \"foo\"\nprint(f\"Defined before function; called outside of function scope: {global_scope_tester}\")\n\ndef print_scope():\n    print(f\"Defined inside function; called outside of function scope: {global_scope_tester}\")\n\n    local_scope_tester = \"bar\"\n    print(f\"Defined inside function; called inside of function scope: {local_scope_tester}\")\n\n    return local_scope_tester\n\nlocal_scope_tester = print_scope()\nprint(f\"Defined inside function; called outside of function scope: {local_scope_tester}\")\n\nThis is the motivation for returning values and writing organized input/output for your functions, keeping scopes and i/o organized leads to readable and consistent code with minimal errors.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Lesson 3 - Abstraction, Functions, and Scope</span>"
    ]
  },
  {
    "objectID": "Lesson_3_Abstraction_Functions/functions_student.html#default-values",
    "href": "Lesson_3_Abstraction_Functions/functions_student.html#default-values",
    "title": "Lesson 3 - Abstraction, Functions, and Scope",
    "section": "Default Values",
    "text": "Default Values\nSometimes you may have a function that will rarely need a different input than a known value. In our addition function, let‚Äôs assume someone almost always wants to add 5 to a value. In situations like these it‚Äôs useful to add a default value in the function definition, this ensures known outcome for the developer but also makes it easier for users to run code if they don‚Äôt want to specify function inputs when the parameter value is (usually) obvious.\nHere‚Äôs a re-definition of the addition machine with a default value of 5 for num_2:\n\ndef add(num_1, num_2=5):\n\n    result = num_1 + num_2\n    \n    return result\n\nadd_res = add(num_1=2)\nprint(add_res)\n\nPretty simple, huh? To set a default value all you need to do is set the input parameter equal to the default value in the definition. Whenever the function is called it now only requires num_1, since the function already knows what num_2 is.\nOf course in this case since addition is commutative (doesn‚Äôt matter which order the numbers are in) this is pretty useless. Let‚Äôs write a subtraction function where the order of inputs matters:\n\ndef subtract(num_1, num_2=5):\n\n    result = num_1 - num_2\n    \n    return result\n\nsubtract_res = subtract(num_1=2)\nprint(subtract_res)\n\nprint(subtract(2, 3))\nprint(subtract(9))\nprint(subtract(num_1=43, num_2=12))\n\nHopefully you can see how default values are useful now, especially when you have a function being used many times. Default values are optional, use your best judgement to add them when they make sense for your function and the context it‚Äôs being used.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Lesson 3 - Abstraction, Functions, and Scope</span>"
    ]
  },
  {
    "objectID": "Lesson_3_Abstraction_Functions/functions_student.html#docstrings",
    "href": "Lesson_3_Abstraction_Functions/functions_student.html#docstrings",
    "title": "Lesson 3 - Abstraction, Functions, and Scope",
    "section": "Docstrings",
    "text": "Docstrings\nYou can imagine that if you‚Äôre writing a lot of functions that do a lot of different things just having the names be different might not be enough to easily figure out what you need quickly. Luckily there is a standardized solution to this: docstrings. These are little descriptor blocks below a function definition to clarify what a function does, what it‚Äôs inputs/outputs are, and any other relevant information (such as stackoverflow citations!)\nThere are multiple docstring format standards, but I prefer the Google style. VSCode even has automatic docstring generators to make the process even easier. Here‚Äôs the template for a Google style docstring:\n\ndef function_with_types_in_docstring(param1, param2):\n    \"\"\"Example function with types documented in the docstring.\n\n    General notes here\n\n    Args:\n        param1 (int): The first parameter.\n        param2 (str): The second parameter.\n\n    Returns:\n        bool: The return value. True for success, False otherwise.\n\n    .. Links:\n        https://www.python.org/dev/peps/pep-0484/\n\n    .. TODO\n        foo\n        bar\n\n    \"\"\"\n\nNote that these types of text blocks can be used for a script as well, so you can have consistently organized code from top to bottom.\nLet‚Äôs go ahead and define all the other functions needed for our calculator with some basic docstrings to exemplify the process:\n\ndef add(num_1, num_2):\n    \"\"\"Add two numbers together.\"\"\"\n    result = num_1 + num_2\n    \n    return result\n\ndef subtract(num_1, num_2):\n    \"\"\"\n    Subtract num_2 from num_1.\n\n    Args:\n        num_1 (Union[int, float]): Number to subtract from.\n        num_2 (Union[int, float]): Number to subtract with.\n\n    Returns:\n        Union[int, float]: Result of subtraction process.\n    \"\"\"\n    result = num_1 - num_2\n    \n    return result\n\ndef multiply(num_1, num_2):\n    \"\"\"Multiply two values together, order doesn't matter.\"\"\"\n    return num_1 * num_2\n    \ndef divide(num_1, num_2):\n    \"\"\"Wait, which number is the numerator and which is the denominator?\"\"\"\n    result = num_1 / num_2\n    \n    return result\n\ndef exponent(num_1, num_2):\n    \"\"\"Take num_1 to the power of num_2\"\"\"\n    result = num_1 ** num_2\n    \n    return result\n\nNow that we have all our components we can build the calculator, remember here is our function design: - Input: - Mathematical operation - Number 1 - Number 2 - Output: - Resulting value from operation - Process (pseudocode): - Check for inputted operation - Calculate the operation using two inputted values - Return resulting value\nKnowing that, we‚Äôll follow this design pretty closely:\n\ndef calculator(operation, num_1, num_2):\n    \"\"\"\n    Calculates a value given a mathematical operation and two values.\n\n    Args:\n        operation (str): Choice of [\"add\", \"subtract\", \"multiply\", \"divide\", \"exponent\"]\n        num_1 (Union[int, float]): Value 1\n        num_2 (Union[int, float]): Value 2\n\n    Returns:\n        Union[int, float]: Value returned from the desired math operation.\n    \"\"\"\n    if operation == \"add\":\n        res = add(num_1, num_2)\n    elif operation == \"subtract\":\n        res = subtract(num_1, num_2)\n    elif operation == \"multiply\":\n        res = multiply(num_1, num_2)\n    elif operation == \"divide\":\n        res = divide(num_1, num_2)\n    elif operation == \"exponent\":\n        res = exponent(num_1, num_2)\n    else:\n        print(\"Bad input\")\n        res = None\n\n    return res\n\ncalculator(\"sum\", 2, 4)\n\nBad input\n\n\nAs you can see this function follows a pretty clear logic: depending on the desired operation it runs the function defined above with the given values, and then returns the value gotten from that. This is meant to demonstrate how functions can be run inside of functions and the results used within the external function. This structure can be done repeatedly for a nested structure, or you can design a process to use a flat structure where you minimally nest function calls. The theory behind which is better is hotly debated, but my rule of thumb is that as long as it follows a consistent logic and you document your functions well nesting can be a great way to chain operations together, such as above.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Lesson 3 - Abstraction, Functions, and Scope</span>"
    ]
  },
  {
    "objectID": "Lesson_3_Abstraction_Functions/functions_student.html#testing",
    "href": "Lesson_3_Abstraction_Functions/functions_student.html#testing",
    "title": "Lesson 3 - Abstraction, Functions, and Scope",
    "section": "Testing",
    "text": "Testing\nNow that we‚Äôve completed our function it‚Äôs time to test it. Testing is very underlooked in the world of scientific computing, but is arguably more important when dealing with high amounts of data and long workflows to ensure consistent results.\nIn Python testing is extremely straightforward with the assert command. This allows you to compare any two values, if they are the same it returns with a value of True, if not it returns a value of False. This allows us to test a given function by writing a ‚Äúunit test‚Äù for the function, where we input a known value with a known outcome and test whether the function responds appropriately.\nHere is a basic example:\n\n#Passes, nothing happens\nassert calculator(\"add\", 2, 2) == 4\n\n\n#Fails, raises an error\nassert calculator(\"add\", 2, 2) == 5\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\n/home/lswhiteh/python-class/Lesson_3_Abstraction_Functions/Lesson_3.ipynb Cell 29' in &lt;cell line: 2&gt;()\n      &lt;a href='vscode-notebook-cell://wsl%2Bubuntu/home/lswhiteh/python-class/Lesson_3_Abstraction_Functions/Lesson_3.ipynb#ch0000031vscode-remote?line=0'&gt;1&lt;/a&gt; #Fails, raises an error\n----&gt; &lt;a href='vscode-notebook-cell://wsl%2Bubuntu/home/lswhiteh/python-class/Lesson_3_Abstraction_Functions/Lesson_3.ipynb#ch0000031vscode-remote?line=1'&gt;2&lt;/a&gt; assert calculator(\"add\", 2, 2) == 5\n\nAssertionError: \n\n\n\nAs you can see when an assertion fails it raises a specific type of error called an AssertionError. If you don‚Äôt want the program to proceed if the test fails, you can just let it error out. If you want to acknowledge it happened but keep going you can use something called a try/catch block, here‚Äôs an example:\n\ntry:\n    assert calculator(\"add\", 2, 2) == 5\nexcept AssertionError:\n    print(\"Test failed!\")\n\nTest failed\n\n\nWhile this is handy for small situations, it‚Äôs preferable to write unit tests for all functions and store them in separate modules for your code. While automated unit testing is outside of the scope of this lecture I highly recommend you read this excellent tutorial on RealPython.\nConsistent and robust unit testing will not only save your code from silently producing erroneous results, it will ensure your code is reproducible even if you make changes.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Lesson 3 - Abstraction, Functions, and Scope</span>"
    ]
  },
  {
    "objectID": "Lesson_3_Abstraction_Functions/functions_student.html#exercises",
    "href": "Lesson_3_Abstraction_Functions/functions_student.html#exercises",
    "title": "Lesson 3 - Abstraction, Functions, and Scope",
    "section": "Exercises",
    "text": "Exercises\nThe following exercises will help you better understand functions, scope, and testing\n\nWrite a function that; given two positional args in first and second position, returns them in the opposite order, i.e.¬†second then first\n\n\n# Question 1 code here\n\n\nWrite a better docstring for the divide() function defined earlier\n\n\n# Question 2 code here\n\n\nWrite a function that prints a subtracts a global variable from a local variable and returns the result, print said result\n\n\n# Question 3 code here\n\n3\n\n\n\nWrite a unit test for any of the calculator sub-functions above, write a conditional print statement based on the output as demonstrated above\n\n\n# Question 4 code here\n\nTesting value: 2\nPassed!\n\nTesting value: 3\nDidn't work with 3!",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Lesson 3 - Abstraction, Functions, and Scope</span>"
    ]
  },
  {
    "objectID": "Lesson_4_FileIO/Lesson_4_student.html",
    "href": "Lesson_4_FileIO/Lesson_4_student.html",
    "title": "Lesson 4 - File IO and string manipulation",
    "section": "",
    "text": "Introduction\nLearning objectives: Students will be able to load text files into Python objects, and learn to manipulate file names and strings.\nSpecific skills:\nEvery program has an input and an output. In science, the input is usually your raw data; the output can be anything from processed data, statistical tests, model predictions, or figures for a paper or presentation. In any case, loading your input data is one of the first tasks that you will have to execute in your code.\nYou have already seen how to load data into a pandas.DataFrame. However, not all data will be formatted this way. This lesson will teach you how to deal with text files, but the tools learned here will be applicable to a variety of different file types that you may come accross in your research.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Lesson 4 - File IO and string manipulation</span>"
    ]
  },
  {
    "objectID": "Lesson_4_FileIO/Lesson_4_student.html#context-management",
    "href": "Lesson_4_FileIO/Lesson_4_student.html#context-management",
    "title": "Lesson 4 - File IO and string manipulation",
    "section": "Context management",
    "text": "Context management\nWriting code to close your file every time you open it is a pain. Fortunately, there is a clean way to deal with this. The Python with statement is known as a context manager, since it is responsible for the context in which the file is being manipulated. What this means, is that the file will automatically be closed once you exit the with statement.\nThis is how it works:\n\nwith open('three_seq.txt') as file_handle:\n    file_contents2 = file_handle.read()\n\n\nfile_contents2\n\n'ATCAGACGCGCAGAGGAGGCGGGGCCGCGGCTGGTTTCCTGCCGGGGGGCGGCTCTGGGCCGCCGAGTCCCCTCCTCCCGCCCCTGAGGAGGAGGAGCCGCCGCCACCCGCCGCGCCCGACACCCGGGAGGCCCCGCCAGCCCGCGGGAGAGGCCCAGCGGGAGTCGCGGAACAGCAGGCCCGAGCCCACCGCGCCGGGCCCCGGACGCCGCGCGGAAAAG\\nCTGCTCCGGAGTGACGCGGGCCCGGGCGCGACGGTCTCGGCGGCGGCGGCGGCGGCGACAGAGCGAGCGCGGCGCGGGGCCACC\\nAGAAGGAGGGCGTGGTAATATGAAGTCAGTTCCGGTTGGTGTAAAACCCCCGGGGCGGCGGCGAACTGGCTTTAGATGCTTCTGGGTCGCGGTGTGCTAAGCGAGGAGTCCGAGTGTGTGAGCTTGAGAGCCGCGCGCTAGAGCGACCCGGCGAGGG'",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Lesson 4 - File IO and string manipulation</span>"
    ]
  },
  {
    "objectID": "Lesson_4_FileIO/Lesson_4_student.html#reading-many-lines",
    "href": "Lesson_4_FileIO/Lesson_4_student.html#reading-many-lines",
    "title": "Lesson 4 - File IO and string manipulation",
    "section": "Reading many lines",
    "text": "Reading many lines\nOftentimes the text file we are using will be organized line by line. In this case, rather than reading the whole thing at once, it makes sense to read each line into a different element of a list.\n\nwith open('three_seq.txt') as handle:\n    seqs = handle.read()\n    seq_list = seqs.splitlines()\n\n\nprint(seq_list)\n\n['ATCAGACGCGCAGAGGAGGCGGGGCCGCGGCTGGTTTCCTGCCGGGGGGCGGCTCTGGGCCGCCGAGTCCCCTCCTCCCGCCCCTGAGGAGGAGGAGCCGCCGCCACCCGCCGCGCCCGACACCCGGGAGGCCCCGCCAGCCCGCGGGAGAGGCCCAGCGGGAGTCGCGGAACAGCAGGCCCGAGCCCACCGCGCCGGGCCCCGGACGCCGCGCGGAAAAG', 'CTGCTCCGGAGTGACGCGGGCCCGGGCGCGACGGTCTCGGCGGCGGCGGCGGCGGCGACAGAGCGAGCGCGGCGCGGGGCCACC', 'AGAAGGAGGGCGTGGTAATATGAAGTCAGTTCCGGTTGGTGTAAAACCCCCGGGGCGGCGGCGAACTGGCTTTAGATGCTTCTGGGTCGCGGTGTGCTAAGCGAGGAGTCCGAGTGTGTGAGCTTGAGAGCCGCGCGCTAGAGCGACCCGGCGAGGG']\n\n\nYou can also chain the commands to fit on a single line:\n\nwith open('three_seq.txt') as handle:\n    seq_list2 = handle.read().splitlines()\n\n\nprint(seq_list2)\n\n['ATCAGACGCGCAGAGGAGGCGGGGCCGCGGCTGGTTTCCTGCCGGGGGGCGGCTCTGGGCCGCCGAGTCCCCTCCTCCCGCCCCTGAGGAGGAGGAGCCGCCGCCACCCGCCGCGCCCGACACCCGGGAGGCCCCGCCAGCCCGCGGGAGAGGCCCAGCGGGAGTCGCGGAACAGCAGGCCCGAGCCCACCGCGCCGGGCCCCGGACGCCGCGCGGAAAAG', 'CTGCTCCGGAGTGACGCGGGCCCGGGCGCGACGGTCTCGGCGGCGGCGGCGGCGGCGACAGAGCGAGCGCGGCGCGGGGCCACC', 'AGAAGGAGGGCGTGGTAATATGAAGTCAGTTCCGGTTGGTGTAAAACCCCCGGGGCGGCGGCGAACTGGCTTTAGATGCTTCTGGGTCGCGGTGTGCTAAGCGAGGAGTCCGAGTGTGTGAGCTTGAGAGCCGCGCGCTAGAGCGACCCGGCGAGGG']",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Lesson 4 - File IO and string manipulation</span>"
    ]
  },
  {
    "objectID": "Lesson_4_FileIO/Lesson_4_student.html#exercises",
    "href": "Lesson_4_FileIO/Lesson_4_student.html#exercises",
    "title": "Lesson 4 - File IO and string manipulation",
    "section": "Exercises",
    "text": "Exercises\nHow would you check that seq_list and seq_list2 are identical?\n\n# your code goes here\n\nWrite a function to read a text file into a list of lines. Test your code using the examples below.\n\n# feel free to change your function name to something more descriptive!\ndef file_reader(path):\n    \n    return \n\n\nfile_reader('dna.txt')[0]\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n/tmp/ipykernel_29734/1649332137.py in &lt;module&gt;\n----&gt; 1 file_reader('dna.txt')[0]\n\nTypeError: 'NoneType' object is not subscriptable\n\n\n\n\nmy_file_test = file_reader('dna.txt')[0]\nassert my_file_test == 'ATATCGCGAA'\nseq_list_test = file_reader('three_seq.txt')\nassert seq_list_test == seq_list",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Lesson 4 - File IO and string manipulation</span>"
    ]
  },
  {
    "objectID": "Lesson_4_FileIO/Lesson_4_student.html#exercises-1",
    "href": "Lesson_4_FileIO/Lesson_4_student.html#exercises-1",
    "title": "Lesson 4 - File IO and string manipulation",
    "section": "Exercises",
    "text": "Exercises\nConvert all nucleotides in ‚Äòdna.txt‚Äô to lowercase.\n\n# your code here\n\nUse f-strings to append a poly-A tail to each sequence in ‚Äòseq_list.txt‚Äô.\n\n# your code here",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Lesson 4 - File IO and string manipulation</span>"
    ]
  },
  {
    "objectID": "Lesson_4_FileIO/Lesson_4_student.html#exercises-2",
    "href": "Lesson_4_FileIO/Lesson_4_student.html#exercises-2",
    "title": "Lesson 4 - File IO and string manipulation",
    "section": "Exercises",
    "text": "Exercises\nUse string manipulation and a for loop or list comprehension to get rid of the version numbers and file extensions in gene_set_files. The output should be identical to gene_set_names.\n\n# your code here\n\nWe don‚Äôt actually need to have ‚ÄòHALLMARK_‚Äô before every gene set name. Use string manipulation and a for loop or list comprehension to remove it.\n\n# your code here",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Lesson 4 - File IO and string manipulation</span>"
    ]
  },
  {
    "objectID": "Lesson_5_Pandas_DataFrame/Lesson5_pandas_DataFrame_Student.html",
    "href": "Lesson_5_Pandas_DataFrame/Lesson5_pandas_DataFrame_Student.html",
    "title": "Lesson 5: Pandas as a dataframe API",
    "section": "",
    "text": "Learning objectives\nStudents will gain a better understanding of how to use pandas for manipulating data frames\nSpecific coding skills learned: - Subsetting data frames - Joining data frames together - Other useful pandas functions (incuding groupby, statistics, and conditional subsetting)\n%pip install pandas \n%pip install matplotlib \n%pip install numpy \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport numpy as np",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Lesson 5: Pandas as a dataframe API</span>"
    ]
  },
  {
    "objectID": "Lesson_5_Pandas_DataFrame/Lesson5_pandas_DataFrame_Student.html#learning-objectives",
    "href": "Lesson_5_Pandas_DataFrame/Lesson5_pandas_DataFrame_Student.html#learning-objectives",
    "title": "Lesson 5: Pandas as a dataframe API",
    "section": "",
    "text": "Introduction\nIn our last class, we discussed the basics of how to use pandas to read in our data. When we load in data from a CSV file with Pandas using a function such as reas_csv, we get a data structure known as a data frame. Basically, this is a two-dimensional table of rows and columns.\nThis is useful in terms of allowing us to visualize our data, but most users will want to utilize the data for their own research purposes. Therefore, users will find it helpful to use pandas as an application programming interface (or API), which is basically a set of functions that allow users access to the features of the data.\nFor this class, we are going to be using daily weather history data from multiple U.S. cities from July 2014 - June 2015 (downloaded from FiveThirtyEight‚Äôs public repository of data found here: https://github.com/fivethirtyeight/data/tree/master/us-weather-history).\nThe first set of data we are going to read in is from Philadelphia.\n\n!wget https://raw.githubusercontent.com/fivethirtyeight/data/master/us-weather-history/KPHL.csv\nphilly_weather = pd.read_csv('KPHL.csv',index_col='date')\nphilly_weather.head()\n\n\n\nSubsetting Data Frames\nThe rows of this data frame represent each day during this period, and the columns represent the weather data that has been collected.\nAs you can see, there are numerous types of data that has been collected, including the temperatures that occurred that day, what the average temperatures have been for that day over the years, the records for that day, and the amount of precipitation that occurred both on that day and on average for that day.\n\nQuestion 1: Let‚Äôs say that you do not care about the temperature data, so most of these columns are not neccessary. How can you get the data you want to have?\n\n\nAnswer: You can subset the data frame to only contain the columns you need\nIn pandas, one of the ways you can select certain subsets of the data is to specify them by the row and/or column names.\nTo select a particular column (for example, the actual preciptation for each day) , you use square brackets [] along with the column name of the column of interest in quotation marks. Alternatively, this is known as ‚Äúslicing‚Äù, since you are taking a slice of the original data frame.\n\nphilly_weather[\"actual_precipitation\"]\n\nIn order to get both columns of precipitation data, we need to use double brackets, then within the brackets we list the columns of interest with a comma between them, as shown below.\n\nphilly_weather[[\"actual_precipitation\",\"average_precipitation\",\"record_precipitation\"]]\n\nAnother way of selecting these columns is to add to the end of our data frame object ‚Äú.loc‚Äù (which looks for the names of the columns by labels). Since our matrix is rows by columns, the ‚Äú:‚Äù symbol tells the computer that we want all of the rows, but only the columns with specific labels.\n\nphilly_weather.loc[:,[\"actual_precipitation\",\"average_precipitation\",\"record_precipitation\"]]\n\nLikewise, we can do the same for the rows, only placing the ‚Äú:‚Äù after the comma (in the columns position), and selecting the rows by label before the comma, as shown below.\n\nphilly_weather.loc[[\"2014-7-1\"],:]\n\n\nphilly_weather.loc[[\"2014-7-1\",\"2014-7-2\"],:]\n\nWhile this is not too difficult if we only need a handful of rows and/or columns, this could become time-consuming if we want to get a series of rows and/or columns.\n\n\nQuestion 2: Instead of using the labels of the rows/columns, what is another attribute you can use to subset the data?\n\n\nAnswer: You can use their numerical location (ex: the 1st row or 2nd column)\nA little earlier in the lesson, we used ‚Äú.loc‚Äù to select rows and columns by their label. To do this by their index, we can instead use ‚Äú.iloc‚Äù to select the row and/or column index value(s).\nGiven that the ‚Äúactual_precipitation‚Äù and ‚Äúaverage_precipitation‚Äù columns represent the 9th through 11th rows respectively, this is how we subset the data to obtain the values in these columns .\n(note: the first piece of code means ‚Äúselect the data from the 9th column up to, but not including, the 12th column‚Äù).\n\nphilly_weather.iloc[:,9:12]\n\n\nphilly_weather.iloc[0:2,:]\n\nIf the numbers are not in a sequence (ex: columns 0, 5, and 7), you can get the subset of the data like this\n\nphilly_weather.iloc[:,[0,5,7]]\n\n\n\n\nJoining Data Frames\nOf course, we might not only be interested in the data from one city. Imagine that we want to do side-by-side comparisons of the average precipitation in Seattle vs.¬†Philadelphia.\nLet‚Äôs first read in the weather pattern data for Seattle as well.\n\n!wget https://raw.githubusercontent.com/fivethirtyeight/data/master/us-weather-history/KSEA.csv\nseattle_weather = pd.read_csv('KSEA.csv',index_col='date')\nseattle_weather.head()\n\nUsing what we learned earlier, we can subset our data to only look at the columns dealing with precipitation:\n\nphilly_weather_precip = philly_weather.iloc[:,9:12]\nseattle_weather_precip = seattle_weather.iloc[:,9:12]\n\nRather than having to flip between both data frames, we can instead combine them into one separate data frame using the function ‚Äújoin‚Äù.\nThere are four different ways that we can join our data, which we will go through below: ‚Äúleft‚Äù, ‚Äúright‚Äù, ‚Äúinner‚Äù, and ‚Äúouter‚Äù\nFor the sake of an example, let us sort our data such that Philadelphia‚Äôs precipitation data is sorted by the lowest to highest record precipitation, while Seattle‚Äôs precipitation data is sorted from the highest to lowest record precipitation.\n\nphilly_weather_precip_reordered = philly_weather_precip.sort_values(by=[\"record_precipitation\"], axis=0, ascending=True)\nphilly_weather_precip_reordered\n\n\nseattle_weather_precip_reordered = seattle_weather_precip.sort_values(by=[\"record_precipitation\"], axis=0, ascending=False)\nseattle_weather_precip_reordered\n\nA left join indicates that we preserve the order of the data frame is getting another data frame joined to it (in this case, the Philadelphia precipitation data). Since the suffix names are the same for both dataframes, we use lsuffix and rsuffix to specify which columns came from which dataframe.\n(Note: ‚Äòleft‚Äô is the default setting)\n\nphilly_weather_precip_reordered.join(seattle_weather_precip_reordered, how ='left', lsuffix = '_philly', rsuffix = '_seattle')\n\nLikewise, a right join indicates that we preserve the order of the data frame that is being joined to the first data frame (in this case, the Seattle precipitation data).\n\nphilly_weather_precip_reordered.join(seattle_weather_precip_reordered, how ='right', lsuffix = '_philly', rsuffix = '_seattle')\n\nIf we want to order the dates lexographically, we can change the how parameter to ‚Äòouter‚Äô (or the union of the datasets).\n\nphilly_weather_precip_reordered.join(seattle_weather_precip_reordered, how ='outer', lsuffix = '_philly', rsuffix = '_seattle')\n\nWhen we combine these datasets together, we can easily compare data from both data frames, for example plotting the record precipitation in Seattle vs.¬†Philadelphia\n\nall_precipitation = philly_weather_precip.join(seattle_weather_precip,lsuffix = '_philly', rsuffix = '_seattle')\nax = plt.gca()\nall_precipitation.plot(kind='line',y='record_precipitation_philly',ax=ax, color = 'purple',figsize=(12,5))\nall_precipitation.plot(kind='line',y='record_precipitation_seattle',ax=ax, color = 'green',figsize=(12,5))\nplt.show()\n\nLet us pretend that we want to add information about each month (stored in the months object)\n\nmonths = pd.Series(['July', 'August','September','October','November','December','January','February','March','April','May','June'])\nmonths = months.repeat([31,31,30,31,30,31,31,28,31,30,31,30])\nmonths = pd.DataFrame(months, columns = ['months'])\nmonths.index = all_precipitation.index\nmonths.index.name = 'date'\nmonths\n\nNow, let us say that in addition to the rainfall data, we also started to collect information about the average dewpoint on each day for the first three months in Philadelphia. However, due to statewide budget cuts scientists could not afford to gather data for the rest of the year.\n\ndewpoint = np.random.randint(40,70,size=92)\ndewpoint = pd.DataFrame(dewpoint, columns=['dewpoint'])\ndewpoint.index = all_precipitation.index[0:92]\ndewpoint.index.name = 'date'\ndewpoint\n\nAs done previously, we can join the datasets together, but you will notice below that all of the dates where there was no dewpoint recorded are labeled ‚ÄòNaN‚Äô (‚ÄúNot a number‚Äù, or undefined, data points).\n\nphilly_weather_precip.join(dewpoint)\n\nWhat if you only want to keep the values where dewpoints are recorded? To do this, you can use join with the how parameter set to ‚Äòinner‚Äô (or intersection of the data), as shown below.\n\nphilly_weather_precip.join(dewpoint, how='inner')\n\n\n\nOther useful pandas functions: conditional subsetting\nThere are other ways to subset datasets by their index or their rownames/colnames.\nLet us say that you are only looking for days where the max temperature was greater than 90. How would we be able to get only those rows?\nTo do this, we can do something called conditional subsetting.\nRemember in lesson 1 when we learned about comparisons, such as ‚Äúgreater than‚Äù ( &gt; ), ‚Äúlesser than‚Äù ( &lt; ), ‚Äúequals‚Äù (==)? We can subset our data this way as well.\nTo pick the rows where the max temperature was greater than 90, we can subset the data as follows\n\nphilly_weather[philly_weather[\"actual_max_temp\"] &gt; 90]\n\nAs another example, what if we wanted to get all the days where it did not rain?\n\nphilly_weather[philly_weather[\"actual_precipitation\"] == 0.00]\n\nWhat if we want to omit years where the high occurred that year? (Hint: remember the boolean operators AND (&) and OR (|)\nAnswer:\n\nphilly_weather[(philly_weather[\"record_max_temp_year\"] != 2014) & (philly_weather[\"record_max_temp_year\"] != 2015)]\n\n\n\nOther useful pandas functions: groupby\nLet us go back to to the all_precip_wmonths object, when we added information about which month each day was in for the combined Philly and Seattle precipitation data. One neat thing we can do is to group data based on certain column values (such as months), as shown below.\n\nall_precip_wmonths.groupby([\"months\"])\n\nWe can do functions such as mean, min, and max for each month, rather than every day.\n\nall_precip_wmonths.groupby([\"months\"]).mean()\n\n\nall_precip_wmonths.groupby([\"months\"]).min()\n\n\nall_precip_wmonths.groupby(['months']).max()\n\nRemember when we plotted the record precipitation in Philadelphia vs.¬†Seattle? What if we instead plotted the mean month-to-month instead of daily?\n\nmean_months_precip = all_precip_wmonths.groupby([\"months\"]).mean()\nax2 = plt.gca()\nmean_months_precip.plot(kind='line',y='record_precipitation_philly',ax=ax2, color = 'purple',figsize=(12,5))\nmean_months_precip.plot(kind='line',y='record_precipitation_seattle',ax=ax2, color = 'green',figsize=(12,5))\nplt.show()\n\nFrom this graph, we can see that months where the record precipitation was on average the highest in Philadelphia (such as August and July) are the lowest precipitation times in Seattle!\n\n\nOther useful pandas functions: statistics\nLastly, we can perform statistics on our data as a whole. We have already gone through how certain mathematical functions such as min, max, or mean can be calculated on different groups, but this can also be done to columns of the data frame as well.\nFor example, we can get the mean value of the actual precipitation in Seattle and Philadelphia, as shown below:\n\nall_precip_wmonths[\"actual_precipitation_seattle\"].mean()\n\n\nall_precip_wmonths[\"actual_precipitation_philly\"].mean()\n\nAs you can see, on average the daily precipitation in Philadelphia is only 0.02 inches greater than Seattle.\nIf you wanted to look at both at the same time, we follow the same rules that we did in label-based subsetting:\n\nall_precip_wmonths[[\"actual_precipitation_seattle\",\"actual_precipitation_philly\"]].mean()\n\nIn general, if you wanted to look at multiple statistics at once, you can call the function ‚Äòdescribe‚Äô, as shown below:\n\nall_precip_wmonths[[\"actual_precipitation_seattle\",\"actual_precipitation_philly\"]].describe()",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Lesson 5: Pandas as a dataframe API</span>"
    ]
  },
  {
    "objectID": "Lesson_5_Pandas_DataFrame/Lesson5_pandas_DataFrame_Student.html#in-class-exercises",
    "href": "Lesson_5_Pandas_DataFrame/Lesson5_pandas_DataFrame_Student.html#in-class-exercises",
    "title": "Lesson 5: Pandas as a dataframe API",
    "section": "In-class exercises",
    "text": "In-class exercises\n\nIn-class exercise 1: How would we select the precipitation data from July 1st and July 2nd, 2014, in Philadelphia, using label-based subsetting?\n\n\nAnswer:\n\n\nIn-class exercise 2: What would we do to only select the precipitation data from July 1st and July 2nd, 2014 using index-based subsetting?\n\n\nAnswer:\n\n\nIn-class exercise 3: Using the ‚Äòall_precipitation‚Äô and ‚Äòmonths‚Äô objects called earlier, how can we join these dataframes together and save it as a new object titled ‚Äòall_precip_wmonths‚Äô?\n\n\nAnswer:\n\n\nIn-class exercise 4: Go back and use the philly_weather and seattle_weather objects to figure out which days had a record minimum tempeature under 10 in either dataset\n\n\nAnswer:\n\n\nIn-class exercise 5: Using the all_precip_wmonths object, how would you find the differences in standard deviation in average precipitation between Philadelphia and Seattle for each month? (Hint: you may want to use the pandas function ‚Äúvalues‚Äù at some point in this answer)\n\n\nAnswer:",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Lesson 5: Pandas as a dataframe API</span>"
    ]
  },
  {
    "objectID": "Lesson_5_Pandas_DataFrame/Lesson5_pandas_DataFrame_Student.html#homework",
    "href": "Lesson_5_Pandas_DataFrame/Lesson5_pandas_DataFrame_Student.html#homework",
    "title": "Lesson 5: Pandas as a dataframe API",
    "section": "Homework",
    "text": "Homework\nThere are 81 different exercises pertaining to manipulating Pandas data frames found here: https://www.w3resource.com/python-exercises/pandas/index-dataframe.php (with the predicted output as well as possible solutions to the problems).\nI would suggest doing problems 3, 4, 5, 7, 9, 10, 12, 13, 14, 15, 24, and 31 (and if you have time/interest, problems 33, 44, 45, 49, and 52 are also good choices)",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Lesson 5: Pandas as a dataframe API</span>"
    ]
  },
  {
    "objectID": "Lesson_6_NumPy/Lesson_6_NumPy_Student_Version.html",
    "href": "Lesson_6_NumPy/Lesson_6_NumPy_Student_Version.html",
    "title": "NumPy",
    "section": "",
    "text": "Learning objectives\nThe python package NumPy is an indespensible tool for efficient data manipulation and numerical processing. It is a highly curated and tested collection of functions centered around the manipulation of thier proprietary data type, the numpy.array. The numpy.array is similar to the standard list type, but it has some important differences that make it particularly useful for generic vector and matrix algebra and element-wise data manipulation.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "Lesson_6_NumPy/Lesson_6_NumPy_Student_Version.html#learning-objectives",
    "href": "Lesson_6_NumPy/Lesson_6_NumPy_Student_Version.html#learning-objectives",
    "title": "NumPy",
    "section": "",
    "text": "What is scientific computing?\nWhat is NumPy?\nWhat are the benefits of NumPy?\nWhat is a numpy.array, and why use it?\nWhere is the NumPy documentation located? Right here!",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "Lesson_6_NumPy/Lesson_6_NumPy_Student_Version.html#whats-the-deal-with-numpy",
    "href": "Lesson_6_NumPy/Lesson_6_NumPy_Student_Version.html#whats-the-deal-with-numpy",
    "title": "NumPy",
    "section": "What‚Äôs the deal with NumPy?",
    "text": "What‚Äôs the deal with NumPy?\nNumPy is the foundational Python package for scientific computing. Scientific computing encompasses any tools, functions, or algorithms that are used to generate or simulate mathematical models and problems. Python in its base form is fine for some data manipulation with support for generic algorithm construction (e.g.¬†loops and function declaration), but as soon as you need to do any mathematics, especially on a large scale (e.g.¬†large data sets), generic Python falls short with respect to efficient built-in functionality. NumPy is a heavily curated and tested package that contains thousands, if not hundreds of thousands, of useful functions for generic mathematical manipulation of your data. This includes general mathematics, like find the natural logarithm, as well as general statistics, like the variance and quanitles. These included functions are also very effienct with respect to compuational time and memory, so they should be used whenever possible.\nSo what makes NumPy so efficient? Part of the efficiency comes from the the fact that the algorithms used behind the scenes are tuned to be as efficient as possible by dedicated computer scientists. Another reason that the code is efficient is that large chunks of the code are written in C and C++ rather than in Python. A pre-compiled C or C++ function is called by NumPy to execute your code, and the results are passed back to you in Python. It is not necessarily important to understand why this is more efficient, but it is important to point out that it is done in this manner because it is more efficient than strictly using Python. They are playing 4-dimensional chess while you are playing checkers, metaphorically, so there is no use in reinvinteing the wheel with respect to creating homemade functions.\nA final reason that NumPy is so efficient is the introduction of the numpy.array. The numpy.array is a generic n-dimensional array object. The easiest way to think about it is that a numpy.array is an n-dimensional matrix which can be used to store and manipulate your data or just do simple matrix and vector algebra. The reason this object is more efficient than the standard Python list is how the memory is stored in each case. In a standard list all of the members are stored in random spots in memory, so when they need to be accessed, the computer has to search around for them. In a numpy.array all of the data points are stored in sequential memory. This means that if you are at a point in your array, my_array[a][b][c], and want to access the adjacent piece of data, my_array[a][b][c+1], the computer can just accesses the adjacent memory address instead of searching through all of the memory for the correct piece of data. In the previous example my_array designates a 3-dimensional array as seen by the three different indexes following the my_array call. Most functions in Numpy are built around manipulating these objects, and are coded in such a way as to take advantage of the memory allocation in an optimal way, and are therefore very efficient.\nIn order to demonstrate the power of NumPy, we can start with the standard dot product. In the code block below we generate two vectors of length 1000 and store them in numpy.array objects and list objects. We create a generic function, my_dot_0, that takes the dot product of two input vectors using a standard for loop. We then time the output.\n\n# np is the standard abbreviation for numpy\nimport numpy as np \n \n# This example uses the random package so setting the seed ensures\n# we get the same result every time we run this block.\nnp.random.seed(5)\n        \n# generic code block to run a dot product with any array like object\ndef my_dot_0(a0, a1):\n    dot = 0\n    for i in range(len(a0)):\n        dot += a0[i] * a1[i]\n    return dot\n\n# create the vectors as numpy arrays\n# np.random.randn is the normal distribution with the number\n# in the parentheses indicating the size of the array you want\nA_arr = np.random.randn(10000)\nB_arr = np.random.randn(10000)\n\n# copy the vectors as lists\nA_list = list(A_arr)\nB_list = list(B_arr)\n\n# %timeit is a built it jupyter notebook function for timing functions easily\n# -n 10 is a statement about how many runs it will use for the time output\n# tmp is a placeholder variable to prevent output clutter\nprint(\"list time for my_dot_0:\")\n%timeit -n 10 tmp = my_dot_0(A_list, B_list)\nprint(\"\\nnp.array time for my_dot_0:\")\n%timeit -n 10 tmp = my_dot_0(A_arr, B_arr)\n\nlist time for my_dot_0:\n3.3 ms ¬± 305 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\n\nnp.array time for my_dot_0:\n4.89 ms ¬± 493 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\n\n\nIn a surprise result, seemingly contrary to everything previously stated, the list manipulation came out faster than the manipulation using the numpy.array. The small caveat is that the numpy.array object is built for speed when used with NumPy‚Äôs built in functions. We can try the test again, but this time we use some built in functions from NumPy to clean up the function definition and improve our results, hopefully.\n\n# using built in NumPy functions\n# np.multiply does element-wise multiplication\n# np.sum adds the elements of the array together \ndef my_dot_1(a0, a1):\n    return np.sum(np.multiply(a0, a1))\n\nprint(\"list time for my_dot_1:\")\n%timeit -n 10 tmp = my_dot_1(A_list, B_list)\nprint(\"\\nnp.array time for my_dot_1:\")\n%timeit -n 10 tmp = my_dot_1(A_arr, B_arr)\n\nlist time for my_dot_1:\n1.49 ms ¬± 171 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\n\nnp.array time for my_dot_1:\n21.8 ¬µs ¬± 14.5 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\n\n\nThat‚Äôs more like it! Notice that the list speed sees some improvement using the built in NumPy functions as well, but certainly not as much as the numpy.array. In general the NumPy functions can handle generic list types, but the full potential is unlocked with the numpy.array type. For this final speed test, we can use the the totally pre-built numpy.dot function and check out the speed.\n\nprint(\"list time for np.dot:\")\n%timeit -n 10 tmp = np.dot(A_list, B_list)\nprint(\"\\nnp.array time for np.dot:\")\n%timeit -n 10 tmp = np.dot(A_arr, B_arr)\n\nlist time for np.dot:\n2.97 ms ¬± 399 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\n\nnp.array time for np.dot:\nThe slowest run took 14.49 times longer than the fastest. This could mean that an intermediate result is being cached.\n15.5 ¬µs ¬± 24.2 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\n\n\nWill you look at that! The totally pre-built function is even faster than our last test with the two different functions. Notice that the list test really struggles, comparitively, with this function. Together, all of these speed tests should highlight: 1. NumPy functions are very efficient. 2. NumPy functions are relatively easy to use and clean up your code. 3. NumPy functions work best with numpy.array objects. 4. If you can find a NumPy function to handle your numpy.array objects, you should use it rather than trying to build your own function.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "Lesson_6_NumPy/Lesson_6_NumPy_Student_Version.html#whats-the-deal-with-numpy-arrays",
    "href": "Lesson_6_NumPy/Lesson_6_NumPy_Student_Version.html#whats-the-deal-with-numpy-arrays",
    "title": "NumPy",
    "section": "What‚Äôs the deal with NumPy arrays?",
    "text": "What‚Äôs the deal with NumPy arrays?\nNow that we know why we should use the humble numpy.array, we should probably learn how to use this object. As mentioned previously is, it is very similar to you standard list with a few built in features that make it easier to use in a a numerical sense. The first feature that is crucial is that unlike a list all of the rows/columns should have the same length, much like any matrix you would normally deal with. This means that numpy.array are best suited for comparing tests or doing data manipulation on data that would fit into a traditional n-dimensional matrix. Below is an example of a data set that is fine in a list but should be avoided in a numpy.array. Python will throw a warning that this functionality is depricated (outdated, no longer supported).\n\nmy_list = [[0, 1, 1], [2, 3]]\nprint(my_list)\nmy_matrix = np.array(my_list)\n\n[[0, 1, 1], [2, 3]]\n\n\nNow that we know what data works best in a numpy.array, we can look into how to build these arrays. In the first example we will look at how to make a (2,3,4) matrix that is filled with 0s for simplicity. In this case we use the built in shape attribute of the array to check its dimensions.\n\n# can make an array of any size filled with \na0 = np.zeros((2, 3, 4)) # just outputs 0s for the shape that you tell it\na1 = np.array([[[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]])\na2 = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\na2.resize((2, 3, 4)) # resize changes the shape of the original array, this can be called on any array\n\n#check that the shapes are equivalent\nprint('a0 shape =',a0.shape,', a1 shape =', a1.shape,', a2 shape =', a2.shape,'\\n')\n\n# sanity check on the dimensions of the above arrays\nprint('the above arrays have dimension', a0.ndim, '\\n')\n\n# this how the resize works with respect to rows/columns in that it starts by filling rows\n# np.arange creates an array with 12 numbers from 0-11\na3 = np.arange(12)\na3.resize((3, 4))\nprint(a3)\n\na0 shape = (2, 3, 4) , a1 shape = (2, 3, 4) , a2 shape = (2, 3, 4) \n\nthe above arrays have dimension 3 \n\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]]\n\n\nAt some point it might also be helpful to sort data within an array, find specific instances of things within an array, delete rows or columns, or add data. This can all be done using NumPy‚Äôs built in functions as well. In the following example we will try to use NumPy to manipulate a data set in a way that would be potentially be useful for your own data. We can also run some simple statistics for fun!\n\n# generate a 5x30 array full or of random numbers\na0 = np.random.randn(5, 300)\n\n# pretend that one of the data points was unfilled, or a NaN\na0[3][5] = np.nan\n\n# we want to be able to find that nan\n# np.isnan sets all instances of nan to true within the array, everything else is false\n# np.where generates a tuple of arrays that show the indices of where the trues are located\n# the tuple that np.where generates will always have the same dimension as the array queried\nwhere_nan = np.where(np.isnan(a0))\n# we are using NaN in this example but you could use any generic conditional to check\nwhere_negative = np.where(a0 &lt; 0.)\n\n# here is a way you could print out the ordered pairs of where the nans were located\nfor i in range(len(where_nan[0])):\n    print('the location of the NaN is',(where_nan[0][i],where_nan[1][i]),'\\n')\n    \n# since the NaN would mess up our data we want to delete the row it is located in\nrow_delete = np.delete(a0, where_nan[0], 0)\n# we could do the same thing if we wanted the column gone instead\ncol_delete = np.delete(a0, where_nan[0], 1)\n# we could also replace the nan with a 0 if we knew what that value should be \n# the tilde (~) flips the boolean values since np.where replaces false not true\n# unlike the previous np.where usage, this outputs our original array with replacements\n# instead of a tuple. This has to do with the arguments fed into the function. \n# The NumPy documentation is wonderful and should be consulted regularly.\nreplace_nan = np.where(~np.isnan(a0), a0, 0.)\n\n# let's plot some of the data we generated\n# first we import the plotting package\n%pip install matplotlib\nimport matplotlib.pyplot as plt\n\n# we then want to plot the distribution of the 2nd and 4th rows\n# we can take extract the rows using standard indexing\n# we then have to take the transpose of the data so that the plt.hist function will read the data correctly\nhist = plt.hist(np.transpose(replace_nan[[1, 3]]), [-3, -2, -1, 0, 1, 2, 3])\nplt.legend([\"$2^{nd}$ row\",\"$4^{th}$ row\"])\n\n# we can output the mean and variance as well to see if it matches the expected 0,1\nmean = np.average(replace_nan[[1 ,3]], axis = 1)\nvariance = np.var(replace_nan[[1, 3]], axis = 1)\nrows = [2, 4]\nfor i in range(2):\n    print('row', rows[i], 'mean =',mean[i], 'and variance =', variance[i])\n\nthe location of the NaN is (3, 5) \n\nrow 2 mean = 0.02053209316073448 and variance = 1.0315188879373587\nrow 4 mean = -0.10027662190124971 and variance = 0.8912458143138923",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "Lesson_7_Plotting/plotting_student.html",
    "href": "Lesson_7_Plotting/plotting_student.html",
    "title": "Plotting in Python",
    "section": "",
    "text": "Learning objectives\nBy the end of the lesson you will learn how to make simple plots using the matplotlib library\nWe‚Äôre going to use matplotlib to accomplish these learning objectives",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Plotting in Python</span>"
    ]
  },
  {
    "objectID": "Lesson_7_Plotting/plotting_student.html#learning-objectives",
    "href": "Lesson_7_Plotting/plotting_student.html#learning-objectives",
    "title": "Plotting in Python",
    "section": "",
    "text": "Basics of plotting\nHow to scale and color\nDifferent types of plots\n\nScatter plots\nBoxplots\nHistograms\n\nmatplotlib axes\n\n\n\nWhat is matplotlib?\nMatplotlib is a Python library used for visualizing data. The website: https://matplotlib.org/stable/users/index has a helpful user guide that contains documentation, information on how to get started (including package installation), an overview of plot types included in the library, and tutorials. If you are stuck or need help figuring out how to make the plot you have in mind, this website is a great place to start.\nMost of the functions that people use from the matplotlib library are in the pyplot submodule and to make it easier to access those functions, it is usually imported as plt.\n\nimport matplotlib.pyplot as plt\n\nTo start plotting, first let‚Äôs create a vector containing values from 0 to 100:\n\nimport numpy as np\nx = np.arange(0, 100)\n\n\nplt.plot(x) #use plot to plot our values\nplt.show() #to show the plot in the notebook\n\nIt assumed our single data list to be the y-values and in the absence of an x-values list, [0, 1, 2, 3, ‚Ä¶ 100] was used instead\n\ny = np.arange(0, 1, 0.01)\n\n\nplt.plot(x, y)\nplt.show()\n\nNow we are passing in two lists to plt.plot, which explicitly sets the x-values\nNOTE: when you pass in two lists to plt.plot, their lengths must match. Here, our lists both contain 100 values\n\ny1 = np.arange(0, 1, 0.01)\ny2 = np.arange(2, 3, 0.01)\n\n\nplt.plot(x, y1)\nplt.plot(x, y2)\nplt.show()\n\nYou can plot multiple curves on the same plot, but they have to be all the same length\nRight now, it‚Äôs hard to tell by glancing which line belongs to which data. We can add labels and show a legend to make it easier to tell them apart\n\nplt.plot(x, y1, label=\"plot 1\")\nplt.plot(x, y2, label=\"plot 2\")\nplt.legend()\nplt.show()\n\nWe can also add labels to the axes and a title\n\nplt.plot(x, y1, label=\"plot 1\")\nplt.plot(x, y2, label=\"plot 2\")\nplt.xlabel(\"x axis\")\nplt.ylabel(\"y axis\")\nplt.title(\"This is a plot of two datasets\")\nplt.legend()\nplt.show()\n\nYou can also change the line type (using linestyle) and color (using color)\n\nplt.plot(x, y1, linestyle=\":\", label=\"line 1\") #include linestyle to change the appearance of the line\nplt.plot(x, y2, color=\"red\", label=\"line 2\") #include color to change the line's color\nplt.xlabel(\"x axis\")\nplt.ylabel(\"y axis\")\nplt.title(\"This is a plot of two datasets\")\nplt.legend()\nplt.show()\n\nYou can change the scaling of either axis by using either xlim or ylim\n\nplt.plot(x, y1, linestyle=\":\", label=\"line 1\")\nplt.plot(x, y2, color=\"red\", label=\"line 2\")\nplt.xlim(0, 60) #sets the lower and upper bound of the x axis\nplt.ylim(0, 2.5) #sets the lower and upper bound of the y axis\nplt.xlabel(\"x axis\")\nplt.ylabel(\"y axis\")\nplt.title(\"This is a plot of two datasets\")\nplt.legend()\nplt.show()\n\nYou can see that this is the same plot as above, except it‚Äôs more zoomed in because we changed the axes",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Plotting in Python</span>"
    ]
  },
  {
    "objectID": "Lesson_7_Plotting/plotting_student.html#other-types-of-plots-in-matplotlib",
    "href": "Lesson_7_Plotting/plotting_student.html#other-types-of-plots-in-matplotlib",
    "title": "Plotting in Python",
    "section": "Other types of plots in matplotlib",
    "text": "Other types of plots in matplotlib\n\n# Scatter plot\nx = np.random.rand(50)\ny = np.random.rand(50)\nplt.scatter(x, y)\nplt.xlabel(\"x axis\")\nplt.ylabel(\"y axis\")\nplt.title(\"Scatter plot\")\nplt.show()\n\n\n# Boxplot\ndata = np.random.rand(50)\nplt.boxplot(data)\nplt.ylabel(\"y axis\")\nplt.title(\"Boxplot\")\nplt.show()\n\n\n# Multiple boxplots\ndata1 = np.random.rand(50)\ndata2 = np.random.rand(50)\n\n# Make a list of the two datasets\ndata = [data1, data2]\nplt.boxplot(data)\nplt.ylabel(\"y axis\")\nplt.title(\"Boxplots for two datasets\")\nplt.show()\n\n\n# Histogram\ndata = np.random.rand(100)\nplt.hist(data, bins=10, facecolor='green', edgecolor='black')\n#edgecolor outlines the edges of the bars with a certain color\nplt.xlabel(\"x axis\")\nplt.ylabel(\"y axis\")\nplt.title(\"Histogram\")\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Plotting in Python</span>"
    ]
  },
  {
    "objectID": "Lesson_8a_Scikit_Learn/scikit_learn_student.html",
    "href": "Lesson_8a_Scikit_Learn/scikit_learn_student.html",
    "title": "Lesson 8a: Scikit-Learn",
    "section": "",
    "text": "Learning objectives\nScikit-learn is a powerful set of tools and techniques for various machine learning (ML) tasks, including classification, regression, clustering, dimensionality reduction, model selection, and data preprocessing.\nBy the end of this lesson, you should have a basic understanding of some of the most common ML techniques as well as how to implement and apply them in Python with scikit-learn.\nFor more information about the other ML techniques as well as more details about those methods discussed here, check out the scikit-learn API: https://scikit-learn.org/stable/modules/classes.html#.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Lesson 8a: Scikit-Learn</span>"
    ]
  },
  {
    "objectID": "Lesson_8a_Scikit_Learn/scikit_learn_student.html#learning-objectives",
    "href": "Lesson_8a_Scikit_Learn/scikit_learn_student.html#learning-objectives",
    "title": "Lesson 8a: Scikit-Learn",
    "section": "",
    "text": "What is machine learning?\nApplying pre-processors and transformers to manipulate and transform input data.\nFitting and evaluating an estimator on your data.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Lesson 8a: Scikit-Learn</span>"
    ]
  },
  {
    "objectID": "Lesson_8a_Scikit_Learn/scikit_learn_student.html#introduction-to-machine-learning",
    "href": "Lesson_8a_Scikit_Learn/scikit_learn_student.html#introduction-to-machine-learning",
    "title": "Lesson 8a: Scikit-Learn",
    "section": "Introduction to Machine Learning",
    "text": "Introduction to Machine Learning\nBefore we dive into various ML techniques and applying them to actual data, let‚Äôs first introduce some datasets that we‚Äôll be using throughout this lesson and some basic ML notions. Datasets can come in various forms and shapes, and the same data can typically be applied to many different learning algorithms. Fortunately, scikit-learn has a few standard datasets built in that we‚Äôll be using: the iris dataset, the digits dataset, and the diabetes dataset. Let‚Äôs examine the iris dataset briefly to introduce some basic concepts.\n\n# Load the iris dataset\nfrom sklearn import datasets\niris = datasets.load_iris()\n\n# Look at the features\nprint(type(iris.data), iris.data.shape)\nprint(iris.data[:5, :])\n\n# Look at the target variables\nprint(type(iris.target), iris.target.shape)\nprint(iris.target_names)\n\n&lt;class 'numpy.ndarray'&gt; (150, 4)\n[[5.1 3.5 1.4 0.2]\n [4.9 3.  1.4 0.2]\n [4.7 3.2 1.3 0.2]\n [4.6 3.1 1.5 0.2]\n [5.  3.6 1.4 0.2]]\n&lt;class 'numpy.ndarray'&gt; (150,)\n['setosa' 'versicolor' 'virginica']\n\n\nThe datasets in scikit-learn are dictionary-like objects that contain the data and some metadata. The data itself is stored in the .data attribute, and as seen above, the iris data is stored as a numpy array of size (150, 4). This corresponds to 150 plants and 4 measurements per plant, forming the ‚Äúdesign matrix‚Äù that is typically of the size (n_samples, n_features). The 150 samples correspond to 150 plants that span 3 different iris species: setosa, versicolor, virginica. These species define our target variables and are also stored in a numpy array of size (150,). Values in this array are integers (0, 1, or 2) corresponding to the species of the plant. Features are attributes or predictors of a sample that are assumed to be predictive for a specific task. For example, the 4 features of the iris dataset correspond to sepal length (cm), sepal width (cm), petal length (cm), and petal width (cm). A physical visualization of these features can be seen below:\n\n\n\nIris Dataset\n\n\nWe can also visualize the distributions of the data and pair-wise scatterplots. (Which features might be the most predictive of iris species? Which might be the least predictive?)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef pair_plot(data, labels, feature_names, label_names, size=(10, 8)):\n\n    # Get shape info of data\n    assert len(data.shape) == 2\n    n_samples = data.shape[0]\n    n_feats = data.shape[1]\n\n    # Separate data and labels based on classes\n    unique_ys = np.unique(labels)\n    X_sep, y_sep = [], []\n    for unique_y in unique_ys:\n        X = [data[i, :] for i in range(n_samples) if labels[i] == unique_y]\n        X_sep.append(np.stack(X))\n\n        y = [labels[i] for i in range(n_samples) if labels[i] == unique_y]\n        y_sep.append(np.stack(y))\n\n    # Plot the pair components\n    fig = plt.figure(figsize=size)\n    for i in range(n_feats):\n        for j in range(n_feats):\n            # If on diagonal, then plot a histogram\n            if i == j:\n                plt.subplot(n_feats, n_feats, n_feats*i+j+1)\n                plt.hist([X[:,j] for X in X_sep])\n            # If off diagonal, then plot a scatter plot\n            else:\n                plt.subplot(n_feats, n_feats, n_feats*i+j+1)\n                for X in X_sep:\n                    plt.scatter(X[:, j], X[:, i])\n            # If on left most column, add ylabel\n            if j == 0:\n                plt.ylabel(feature_names[i])\n            # If on bottom most row, add xlabel\n            if i == n_feats-1:\n                plt.xlabel(feature_names[j])\n    \n    # Add legend and show\n    fig.legend([label_name for label_name in label_names], loc='center right')\n    plt.show()\n\npair_plot(iris.data, iris.target, iris.feature_names, iris.target_names)\n\n\n\n\n\n\n\n\nA typical ML problem for this dataset might be defined as follows: Given the measurements of sepal length, sepal width, petal length, and petal width from an iris plant, predict the specific iris species of that plant.\nThe problem defined above falls into the category of supervised learning, whereby we use the features of an example to predict a certain target variable for that example. In our iris dataset example, the target variable is species. Because our target variable falls into one of 3 distinct classes (setosa, versicolour, or virginica), this is also a classification problem, wherein we try to predict a qualitative value or assign an example to a specific class. Another supervised learning problem is regression, wherein we try to predict a quantitative value for an example. For the iris plants, one might imagine using the same measurements to try to predict the age of the plant. (Of course, we‚Äôd need a new dataset for this problem. Why?)\nAnother category of ML problems is unsupervised learning. Here, the task is a little less defined and more ambiguous. In theory, the idea is to learn about some patterns and/or structure within the data without the use of a specific target variable. For example, imagine we were just given iris.data and there were no iris.target. We can then use an unsupervised learning approach, such as clustering, to try to figure out if there is any structure to our data. Clustering, for instance, attempts to form groups of examples that are grouped based on similarity, wherein more similar examples are grouped together and dissimilar examples are in separate groups.\nNote that a ML technique or algorithm does not have to cleanly fit into one of these categories. There are many examples of methods that combine supervised and unsupervised learning as well as regression and classification.\nThe goal of ML is learn some properties of a dataset and apply these learned properties to unseen or new data. In this mindset, we want to learn something about the data we have that is generalizable to future data. In order to approximate how well our model or technique is performing, it is standard to create a training set and a testing set. The training set is the data that the model or ML technique actually sees and can learn from. The testing set is then used to see how well the learned properties correspond to real unseen data. Scikit-learn has a convenient utility function to do just this:\n\nfrom sklearn.model_selection import train_test_split\n\n# Get our features and targets\nX, y = iris.data, iris.target\n\n# Shuffle and split our dataset with 25% of it going to the testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=True)\n\n# Examine the shapes of the split dataset\nprint(X_train.shape, X_test.shape)\nprint(y_train.shape, y_test.shape)\n\n(112, 4) (38, 4)\n(112,) (38,)",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Lesson 8a: Scikit-Learn</span>"
    ]
  },
  {
    "objectID": "Lesson_8a_Scikit_Learn/scikit_learn_student.html#preprocessing-data",
    "href": "Lesson_8a_Scikit_Learn/scikit_learn_student.html#preprocessing-data",
    "title": "Lesson 8a: Scikit-Learn",
    "section": "Preprocessing Data",
    "text": "Preprocessing Data\nUnfortunately, real world data isn‚Äôt as clean as the datasets that we can load from sklearn.datasets. Fortunately, there are some built-in data preprocessing utilities that we can use to help us prepare our data for various ML techniques. Here we‚Äôll briefly introduce five of these utilities: the StandardScaler, the Normalizer, the OrdinalEncoder, the KBinsDiscretizer, and PolynomialFeatures.\n(Note that in these examples, .fit() and then .transform() is used. An alternative to this would be the .fit_transform() method which fits and transforms the data and the same time.)\n\nStandardScaler\nStandardization is a typical requirement for many ML techniques as they assume the data looks standard normally distributed. While the true distribution is commonly not Gaussian, we often ignore this and just center the data by subtracting the mean for each feature and scaling them by their standard deviation. This can be easily accomplished with the StandardScaler.\nThe StandardScaler must be ‚Äúfit‚Äù to data using the .fit() method. This method will compute the mean and standard deviation of the input data and store these within the StandardScaler object. These values can then be used to transform the input data and any other data with the .transform() method. (Should we create two different StandardScaler and scale the training and testing set separately? Why or why not?)\n\nfrom sklearn import preprocessing\nimport numpy as np\n\n# Create some fake training data\nX_train = np.array([[1., -1.,  2.],\n                    [2.,  0.,  0.],\n                    [0.,  1., -1.]])\nprint('Original data mean:', X_train.mean(axis=0))\nprint('Original data std dev:', X_train.std(axis=0))\n\n# Create the StandardScaler and fit to the data\nscaler = preprocessing.StandardScaler().fit(X_train)\n\n# View the stored scaling values\nprint('Mean: ', scaler.mean_)\nprint('Std. dev: ', scaler.scale_)\n\n# Apply the same transformation to other data\nX_scaled = scaler.transform(X_train)\n\n# Verify the result\nprint('Scaled data mean:', X_scaled.mean(axis=0))\nprint('Scaled data std dev:', X_scaled.std(axis=0))\n\nOriginal data mean: [1.         0.         0.33333333]\nOriginal data std dev: [0.81649658 0.81649658 1.24721913]\nMean:  [1.         0.         0.33333333]\nStd. dev:  [0.81649658 0.81649658 1.24721913]\nScaled data mean: [0. 0. 0.]\nScaled data std dev: [1. 1. 1.]\n\n\n\n\nNormalization\nNormalization another common technique and involves scaling individual samples to have unit norm. This is useful for techniques that utilize a quadratic form or applying kernels to quantify similarity between pairs of samples and normalization is a base assumption for certain techniques.\nThe Normalizer can be used to easily transform samples based on various norms including the \\(\\ell_{1}\\) , \\(\\ell_{2}\\), and max norms. When creating the Normalizer, it does not need the .fit() method (though it can be used, this method doesn‚Äôt do anything because the class is stateless). The .transform() method can then be used to normalize any data. (Should we create two different Normalizer and normalize the training and testing set separately? Why or why not?)\n\n# Create some fake training data\nX_train = [[1., -1.,  2.],\n           [2.,  0.,  0.],\n           [0.,  1., -1.]]\n\n# Create l2 Normalizer (fit does nothing)\nnormalizer_l2_fit = preprocessing.Normalizer(norm='l2').fit(X_train)\nnormalizer_l2 = preprocessing.Normalizer(norm='l2')\n\n# Transform based on l2 norm\nX_norm1 = normalizer_l2_fit.transform(X_train)\nX_norm2 = normalizer_l2.transform(X_train)\nprint('L2 Normalized:')\nprint(X_norm1)\nprint(X_norm2)\nprint()\n\n# Can also create l1 and max norm Normalizer\nnormalizer_l1 = preprocessing.Normalizer(norm='l1')\nnormalizer_max = preprocessing.Normalizer(norm='max')\n\n# Transform based on l1 and max norms\nX_l1 = normalizer_l1.transform(X_train)\nprint('L1 Normalized:')\nprint(X_l1)\nprint()\n\nX_max = normalizer_max.transform(X_train)\nprint('Max Normalized:')\nprint(X_max)\n\nL2 Normalized:\n[[ 0.40824829 -0.40824829  0.81649658]\n [ 1.          0.          0.        ]\n [ 0.          0.70710678 -0.70710678]]\n[[ 0.40824829 -0.40824829  0.81649658]\n [ 1.          0.          0.        ]\n [ 0.          0.70710678 -0.70710678]]\n\nL1 Normalized:\n[[ 0.25 -0.25  0.5 ]\n [ 1.    0.    0.  ]\n [ 0.    0.5  -0.5 ]]\n\nMax Normalized:\n[[ 0.5 -0.5  1. ]\n [ 1.   0.   0. ]\n [ 0.   1.  -1. ]]\n\n\n\n\nOrdinalEncoder\nCommonly some features are not continuous or even quantitative values, but rather categorical. For example, a person could be from [‚ÄúNorth Carolina‚Äù, ‚ÄúSouth Carolina‚Äù, ‚ÄúGeorgia‚Äù, ‚ÄúVirginia‚Äù, ‚ÄúFlorida‚Äù, etc.]. These features can be conveniently transformed to a new feature of integers from 0 to n_categories-1. (Why can‚Äôt we just use the original categorical features?)\nThe OrdinalEncoder can be used to efficiently transform any input categorical features. Like the other preprocessors, we use the .fit() and .transform() methods to do the transformation. There is additional methods to perform the inverse transformation.\n\n# Create fake training data\nX = [['Male', 1], ['Female', 3], ['Female', 2]]\n\n# Create encoder\nencoder = preprocessing.OrdinalEncoder().fit(X)\n\n# View the encoding categories\nprint('Categories:', encoder.categories_)\n\n# Transform new data\nX_encoded = encoder.transform([['Female', 3], ['Male', 1]])\nprint('Encoded data:', X_encoded)\n\n# Perform inverse transformation\nprint('Inverse transformation:', encoder.inverse_transform([[1, 0], [0, 1]]))\n\nCategories: [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\nEncoded data: [[0. 2.]\n [1. 0.]]\nInverse transformation: [['Male' 1]\n ['Female' 2]]\n\n\n\n\nK-Bins Discretization\nDiscretization is a common technique where by continuous values are binned or partitioned into discrete values. One-hot encoded discretized features, for example, can make some models more expressive.\nThe KBinsDiscretizer can be used to easily discretize features into \\(k\\) bins. The actual encoding of the features can be set as a parameter and as default is one-hot encoding. (Here we also show ‚Äòordinal‚Äô encoding). Like the other preprocessors, the KBinsDiscretizer can be .fit() and then can use .transform() to discretize features. Note that when using one-hot encoding, the output will be stored in a sparse matrix representation.\n\n# Create fake data\nX = np.array([[-3., 5.],\n              [ 0., 6.]])\n\n# One hot encoding\none_hot_binner = preprocessing.KBinsDiscretizer(n_bins=[3, 2]).fit(X)\nX_one_hot = one_hot_binner.transform(X)\nprint('One hot encoding:', X_one_hot)\n\n# Ordinal encoding\nordinal_binner = preprocessing.KBinsDiscretizer(n_bins=[3, 2], encode='ordinal').fit(X)\nX_ordinal = ordinal_binner.transform(X)\nprint('Ordinal encoding:', X_ordinal)\n\nOne hot encoding:   (0, 0)  1.0\n  (0, 3)    1.0\n  (1, 2)    1.0\n  (1, 4)    1.0\nOrdinal encoding: [[0. 0.]\n [2. 1.]]\n\n\n\n\nPolynomial Features\nOften, higher order and interaction terms of the features can be useful for many ML techniques. These can be easily acquired via PolynomialFeatures. In the example below, features of a sample \\((X_{1}, X_{2})\\) are transformed to \\((1, X_{1}, X_{2}, X_{1}^{2}, X_{1}X_{2}, X_{2}^{2})\\).\n\nimport numpy as np\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Create fake data\nX = np.arange(6).reshape(3, 2)\nprint(X)\n\n# Create PolynomialFeatures\npoly = PolynomialFeatures(2)\n\n# Transform the features\nX_poly = poly.fit_transform(X)\nprint(X_poly)\n\n[[0 1]\n [2 3]\n [4 5]]\n[[ 1.  0.  1.  0.  0.  1.]\n [ 1.  2.  3.  4.  6.  9.]\n [ 1.  4.  5. 16. 20. 25.]]",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Lesson 8a: Scikit-Learn</span>"
    ]
  },
  {
    "objectID": "Lesson_8a_Scikit_Learn/scikit_learn_student.html#machine-learning-algorithms",
    "href": "Lesson_8a_Scikit_Learn/scikit_learn_student.html#machine-learning-algorithms",
    "title": "Lesson 8a: Scikit-Learn",
    "section": "Machine Learning Algorithms",
    "text": "Machine Learning Algorithms\nLet‚Äôs dive into some common machine learning algorithms, specifically linear regression, logistic regression, k-nearest neighbors, support vector machines, and random forests. Before we get started, let‚Äôs load some datasets. We‚Äôll be using the iris dataset that we introduced earlier for various classification tasks. For regression tasks, we‚Äôll be using the diabetes dataset. For both these datasets, we‚Äôll use a training/testing split of 75%/25%.\n\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\n# Construct training and testing sets for iris dataset\niris = datasets.load_iris()\nX_clf, y_clf = iris.data, iris.target\nX_clf_trn, X_clf_tst, y_clf_trn, y_clf_tst = train_test_split(X_clf, y_clf, test_size=0.25, shuffle=True)\n\n# Construct training and testing sets for diabetes dataset\ndiabetes = datasets.load_diabetes()\nX_reg, y_reg = diabetes.data, diabetes.target\nX_reg_trn, X_reg_tst, y_reg_trn, y_reg_tst = train_test_split(X_reg, y_reg, test_size=0.25, shuffle=True)\n\n\nLinear Regression\nLinear regression is a classic machine learning algorithm that fits a linear combination of the features to a continuous value. As the name says, this technique is for regression problems, and as such, we‚Äôll be applying it to the diabetes dataset. The basic form of linear regression attempts to find good coefficient values for a model of the form: \\[ y = \\beta_{0} + \\beta_{1} X_{1} + \\beta_{2} X_{2} + \\dots + \\beta_{p} X_{p} + \\varepsilon,\\] where \\(y\\) is the regression target, \\(\\beta_{i}\\) for \\(i \\in \\{0 \\dots p\\}\\) are the regression coefficients, \\(X_{i}\\) for \\(i \\in \\{1 \\dots p\\}\\) are the features, and \\(\\varepsilon\\) is the irreducible error.\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Perform the regression\nlin_reg = LinearRegression().fit(X_reg_trn, y_reg_trn)\n\n# Predict on test set\ny_pred = lin_reg.predict(X_reg_tst)\n\n# MSE\nprint('Mean squared error: \\n %.2f' % mean_squared_error(y_reg_tst, y_pred))\n# Coefficient of determination (R^2)\nprint('R squared: \\n %.2f' % r2_score(y_reg_tst, y_pred))\n\nMean squared error: \n 2424.09\nR squared: \n 0.52\n\n\n\n\nLogistic Regression\nLogistic regression is another classic machine learning algorithm that fits a linear combination of the features to a continuous value within the range of 0 and 1. However, unlike as the name suggests, this technique is used for classification problems, and as such, we‚Äôll be applying it to the iris dataset. The basic form of logistic regression attempts to find good coefficient values for a model of the form: \\[ \\frac{p(X)}{1-p(X)} = e^{\\beta_{0} + \\beta_{1} X_{1} + \\beta_{2} X_{2} + \\dots + \\beta_{p} X_{p} + \\varepsilon},\\] where \\(p(X)\\) is the probability of \\(X\\), \\(\\beta_{i}\\) for \\(i \\in \\{0 \\dots p\\}\\) are the regression coefficients, \\(X_{i}\\) for \\(i \\in \\{1 \\dots p\\}\\) are the features, and \\(\\varepsilon\\) is the irreducible error.\n\nfrom sklearn.linear_model import LogisticRegression\n\n# Perform the logistic regression\nlog_reg = LogisticRegression(random_state=0).fit(X_clf_trn, y_clf_trn)\n\n# Predict on test set\ny_pred = log_reg.predict(X_clf_tst)\n\n# Mean accuracy\nprint('Mean accuracy: \\n', log_reg.score(X_clf_tst, y_clf_tst))\n\nMean accuracy: \n 1.0\n\n\n\n\nK-Nearest Neighbors\nK-nearest neighbors is a simple non-parametric machine learning algorithm. As suggested by the name, when the algorithm sees a new example, it then looks at the \\(k\\) closest neighbors (based on some distance metric) to determine a value for the example. The determined value depends if the problem is a classification problem or a regression problem. For classification, the method takes a simple majority vote to assign a class. For regression, the method typically takes an average of the value of the \\(k\\) neighbors.\n\nK-Nearest Neighbors Classifier\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Get the classifier\nknn_clf = KNeighborsClassifier(n_neighbors=3).fit(X_clf_trn, y_clf_trn)\n\n# Predict on test set\ny_pred = knn_clf.predict(X_clf_tst)\n\n# Mean accuracy\nprint('Mean accuracy: \\n %.2f' % knn_clf.score(X_clf_tst, y_clf_tst))\n\nMean accuracy: \n 1.00\n\n\n\n\nK-Nearest Neighbors Regressor\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\n\n# Get the regressor\nknn_reg = KNeighborsRegressor(n_neighbors=5).fit(X_reg_trn, y_reg_trn)\n\n# Predict on test set\ny_pred = knn_reg.predict(X_reg_tst)\n\n# Mean squared error\nprint('Mean squared error: \\n %.2f' % mean_squared_error(y_reg_tst, y_pred))\n# Coefficient of determination\nprint('R squared: \\n %.2f' % knn_reg.score(X_reg_tst, y_reg_tst))\n\nMean squared error: \n 3371.64\nR squared: \n 0.33\n\n\n\n\n\nSupport Vector Machine\nSupport vector machines are supervised learning models that were originally built for classification. For classification, it attempts to fit n_classes-1 hyperplanes between the n_classes classes while maximizing the width of the gap between classes. A regression version of the SVM was later proposed.\n\nSupport Vector Classifier\n\nfrom sklearn.svm import SVC\n\n# Create the support vector classifier with linear kernel\nsvc_1 = SVC(kernel='linear').fit(X_clf_trn, y_clf_trn)\n\n# Create the support vector classifier with RBF kernel\nsvc_2 = SVC(kernel='rbf').fit(X_clf_trn, y_clf_trn)\n\n# Predict on test set\ny_pred_1 = svc_1.predict(X_clf_tst)\ny_pred_2 = svc_2.predict(X_clf_tst)\n\n# Mean accuracy\nprint('Mean accuracy (linear): \\n %.2f' % svc_1.score(X_clf_tst, y_clf_tst))\nprint('Mean accuracy (rbf): \\n %.2f' % svc_2.score(X_clf_tst, y_clf_tst))\n\nMean accuracy (linear): \n 1.00\nMean accuracy (rbf): \n 0.95\n\n\n\n\nSupport Vector Regressor\n\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\n\n# Create the support vector regressor with linear kernel\nsvr_1 = SVR(kernel='linear').fit(X_reg_trn, y_reg_trn)\n\n# Create the support vector regressor with RBF kernel\nsvr_2 = SVR(kernel='rbf').fit(X_reg_trn, y_reg_trn)\n\n# Predict on test set\ny_pred_1 = svr_1.predict(X_reg_tst)\ny_pred_2 = svr_2.predict(X_reg_tst)\n\n# MSE\nprint('Mean squared error (linear): \\n %.2f' % mean_squared_error(y_reg_tst, y_pred_1))\nprint('Mean squared error (rbf): \\n %.2f' % mean_squared_error(y_reg_tst, y_pred_2))\n\n# Coefficient of determination, R^2\nprint('R squared (linear): \\n %.2f' % svr_1.score(X_reg_tst, y_reg_tst))\nprint('R squared (rbf): \\n %.2f' % svr_2.score(X_reg_tst, y_reg_tst))\n\nMean squared error (linear): \n 5282.97\nMean squared error (rbf): \n 4429.20\nR squared (linear): \n -0.05\nR squared (rbf): \n 0.12\n\n\n\n\n\nRandom Forests\nRandom forests are ensemble-based machine learning algorithms that fit many different decision trees to the data, with each tree restricted to a subset of predictors. Both classification and regression versions of random forests have been implemented in scikit-learn.\n\nRandom Forest Classifier\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Build the random forest\nrf_clf = RandomForestClassifier().fit(X_clf_trn, y_clf_trn)\n\n# Predict on test set\ny_pred = rf_clf.predict(X_clf_tst)\n\n# Accuracy metric\nprint('Mean accuracy: \\n %.2f' % accuracy_score(y_clf_tst, y_pred))\n\nMean accuracy: \n 0.97\n\n\n\n\nRandom Forest Regressor\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\n# Build the random forest\nrf_reg = RandomForestRegressor().fit(X_reg_trn, y_reg_trn)\n\n# Predict on test set\ny_pred = rf_reg.predict(X_reg_tst)\n\n# MSE\nprint('Mean squared error: \\n %.2f' % mean_squared_error(y_reg_tst, y_pred))\n# Coefficient of determination\nprint('R squared: \\n %.2f' % rf_reg.score(X_reg_tst, y_reg_tst))\n\nMean squared error: \n 3128.73\nR squared: \n 0.38",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Lesson 8a: Scikit-Learn</span>"
    ]
  },
  {
    "objectID": "Lesson_8b_Python_Ecosystem/Lesson_8b_Python_Ecosystem_teacher.html",
    "href": "Lesson_8b_Python_Ecosystem/Lesson_8b_Python_Ecosystem_teacher.html",
    "title": "Lesson 8 - Python Ecosystem",
    "section": "",
    "text": "Learning objectives\nDon‚Äôt reinvent the wheel.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Lesson 8 - Python Ecosystem</span>"
    ]
  },
  {
    "objectID": "Lesson_8b_Python_Ecosystem/Lesson_8b_Python_Ecosystem_teacher.html#learning-objectives",
    "href": "Lesson_8b_Python_Ecosystem/Lesson_8b_Python_Ecosystem_teacher.html#learning-objectives",
    "title": "Lesson 8 - Python Ecosystem",
    "section": "",
    "text": "Importing packages\nInstalling packages with pip\nIntroduction to conda",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Lesson 8 - Python Ecosystem</span>"
    ]
  },
  {
    "objectID": "Lesson_8c_scanpy/Lesson_8c_scanpy.html",
    "href": "Lesson_8c_scanpy/Lesson_8c_scanpy.html",
    "title": "scanpy",
    "section": "",
    "text": "Learning objectives\nNote: This notebook was created largely based off Preprocessing and clustering 3k PBMCs. Portions have been removed/edited to adapt to a class time of ~30 minutes.\nscanpy is a toolkit based in python for single-cell analysis. Some applications of scanpy include:\nLet‚Äôs first discuss what single-cell data is/looks like‚Ä¶",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>[scanpy](https://scanpy.readthedocs.io/en/stable/)</span>"
    ]
  },
  {
    "objectID": "Lesson_8c_scanpy/Lesson_8c_scanpy.html#single-cell-data",
    "href": "Lesson_8c_scanpy/Lesson_8c_scanpy.html#single-cell-data",
    "title": "scanpy",
    "section": "single-cell data",
    "text": "single-cell data\nThere are several types of single-cell data:\n\nscDNA-seq (genomic single-cell)\nscRNA-seq (transcriptomic single-cell)\nscBS-seq (single-cell bisulfite sequencing)\n‚Ä¶\n\nThese modalities differentiate biological behavior/mechanisms. In this tutorial, we will be looking at 2700 peripheral blood mononuclear cells (PBMCs) from a healthy donor.\nWhat are the benefits of single-cell sequencing over bulk sequencing?\nKnowing the sequencing profiles of single-cells adds granularity to data obtained from samples that may contain more than one type of cell. For instance, knowing the transcriptomic profiles of single cells in a population of heterogeneous tumor cells can reveal insights into tumorigenesis. Researchers may be able to investigate novel biological activity as stem cells abnormally mature into cancerous cells.\nHow is single-cell data created?\nThere are various methods depending on the application and type of single-cell data. However, I hope the image (Chan Zuckerberg Initiative) below captures how single-cell data is created. In short, single cells are isolated from tissue, sequenced and amplified.\n\n\n\nscRNA-seq workflow\n\n\nNow that we know how single-cell data is generated, let‚Äôs talk about how single-cell data is represented in scanpy.\n\n# install these packages first\n# install the anndata library\n!pip install anndata\n# install the scanpy library\n!pip install scanpy\n!pip install leidenalg\n\nCollecting anndata\n  Downloading anndata-0.9.2-py3-none-any.whl (104 kB)\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0.0/104.2 kB ? eta -:--:--     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫ 102.4/104.2 kB 2.7 MB/s eta 0:00:01     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 104.2/104.2 kB 1.7 MB/s eta 0:00:00\nRequirement already satisfied: pandas!=2.0.1,&gt;=1.1.1 in /usr/local/lib/python3.10/dist-packages (from anndata) (1.5.3)\nRequirement already satisfied: numpy&gt;=1.16.5 in /usr/local/lib/python3.10/dist-packages (from anndata) (1.22.4)\nRequirement already satisfied: scipy&gt;1.4 in /usr/local/lib/python3.10/dist-packages (from anndata) (1.10.1)\nRequirement already satisfied: h5py&gt;=3 in /usr/local/lib/python3.10/dist-packages (from anndata) (3.8.0)\nRequirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from anndata) (8.3.1)\nRequirement already satisfied: packaging&gt;=20 in /usr/local/lib/python3.10/dist-packages (from anndata) (23.1)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.0.1,&gt;=1.1.1-&gt;anndata) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.0.1,&gt;=1.1.1-&gt;anndata) (2022.7.1)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.8.1-&gt;pandas!=2.0.1,&gt;=1.1.1-&gt;anndata) (1.16.0)\nInstalling collected packages: anndata\nSuccessfully installed anndata-0.9.2\nCollecting scanpy\n  Downloading scanpy-1.9.3-py3-none-any.whl (2.0 MB)\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2.0/2.0 MB 7.7 MB/s eta 0:00:00\nRequirement already satisfied: anndata&gt;=0.7.4 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.9.2)\nRequirement already satisfied: numpy&gt;=1.17.0 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.22.4)\nRequirement already satisfied: matplotlib&gt;=3.4 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.7.1)\nRequirement already satisfied: pandas&gt;=1.0 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.5.3)\nRequirement already satisfied: scipy&gt;=1.4 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.10.1)\nRequirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.12.2)\nRequirement already satisfied: h5py&gt;=3 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.8.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from scanpy) (4.65.0)\nRequirement already satisfied: scikit-learn&gt;=0.22 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.2.2)\nRequirement already satisfied: statsmodels&gt;=0.10.0rc2 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.13.5)\nRequirement already satisfied: patsy in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.5.3)\nRequirement already satisfied: networkx&gt;=2.3 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.1)\nRequirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from scanpy) (8.3.1)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.3.1)\nRequirement already satisfied: numba&gt;=0.41.0 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.56.4)\nCollecting umap-learn&gt;=0.3.10 (from scanpy)\n  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 88.2/88.2 kB 8.0 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from scanpy) (23.1)\nCollecting session-info (from scanpy)\n  Downloading session_info-1.0.0.tar.gz (24 kB)\n  Preparing metadata (setup.py) ... done\nRequirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.4-&gt;scanpy) (1.1.0)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.4-&gt;scanpy) (0.11.0)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.4-&gt;scanpy) (4.41.1)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.4-&gt;scanpy) (1.4.4)\nRequirement already satisfied: pillow&gt;=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.4-&gt;scanpy) (9.4.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.4-&gt;scanpy) (3.1.0)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.4-&gt;scanpy) (2.8.2)\nRequirement already satisfied: llvmlite&lt;0.40,&gt;=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba&gt;=0.41.0-&gt;scanpy) (0.39.1)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba&gt;=0.41.0-&gt;scanpy) (67.7.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=1.0-&gt;scanpy) (2022.7.1)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn&gt;=0.22-&gt;scanpy) (3.2.0)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy-&gt;scanpy) (1.16.0)\nCollecting pynndescent&gt;=0.5 (from umap-learn&gt;=0.3.10-&gt;scanpy)\n  Downloading pynndescent-0.5.10.tar.gz (1.1 MB)\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1.1/1.1 MB 16.3 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\nCollecting stdlib_list (from session-info-&gt;scanpy)\n  Downloading stdlib_list-0.9.0-py3-none-any.whl (75 kB)\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 75.6/75.6 kB 6.8 MB/s eta 0:00:00\nBuilding wheels for collected packages: umap-learn, session-info, pynndescent\n  Building wheel for umap-learn (setup.py) ... done\n  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82813 sha256=d621e93e424f222cf64e049770dc68c73ebfec399f317778321e149dd0c7824f\n  Stored in directory: /root/.cache/pip/wheels/a0/e8/c6/a37ea663620bd5200ea1ba0907ab3c217042c1d035ef606acc\n  Building wheel for session-info (setup.py) ... done\n  Created wheel for session-info: filename=session_info-1.0.0-py3-none-any.whl size=8026 sha256=9397315748180130ebfce6f3d344da6b1d84b55c7fb8765365998deddbbb0b09\n  Stored in directory: /root/.cache/pip/wheels/6a/aa/b9/eb5d4031476ec10802795b97ccf937b9bd998d68a9b268765a\n  Building wheel for pynndescent (setup.py) ... done\n  Created wheel for pynndescent: filename=pynndescent-0.5.10-py3-none-any.whl size=55621 sha256=4479dfebf6a5e21ef5dcbbeeca68e8e9d88e3cbd9d806052d9f71dd5a4b38356\n  Stored in directory: /root/.cache/pip/wheels/4a/38/5d/f60a40a66a9512b7e5e83517ebc2d1b42d857be97d135f1096\nSuccessfully built umap-learn session-info pynndescent\nInstalling collected packages: stdlib_list, session-info, pynndescent, umap-learn, scanpy\nSuccessfully installed pynndescent-0.5.10 scanpy-1.9.3 session-info-1.0.0 stdlib_list-0.9.0 umap-learn-0.5.3\nCollecting leidenalg\n  Downloading leidenalg-0.10.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2.0/2.0 MB 10.5 MB/s eta 0:00:00\nCollecting igraph&lt;0.11,&gt;=0.10.0 (from leidenalg)\n  Downloading igraph-0.10.6-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3.3/3.3 MB 25.1 MB/s eta 0:00:00\nCollecting texttable&gt;=1.6.2 (from igraph&lt;0.11,&gt;=0.10.0-&gt;leidenalg)\n  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\nInstalling collected packages: texttable, igraph, leidenalg\nSuccessfully installed igraph-0.10.6 leidenalg-0.10.1 texttable-1.6.7\n\n\n\n# import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport anndata as ad\nfrom scipy.sparse import csr_matrix\nimport scanpy as sc\n\n ## AnnData\nThe anndata python package enables the use of anndata objects which are essentially annotated data matrices.\nThe following (taken from Getting started with anndata) demonstrates some of the features of the anndata object.\n\n# first, create a matrix of 100 (cells) x 2000 (genes)\ncounts = csr_matrix(np.random.poisson(1, size=(100, 2000)), dtype=np.float32)\nadata = ad.AnnData(counts)\n# create index names...observation names and variable names\nadata.obs_names = [f\"Cell_{i:d}\" for i in range(adata.n_obs)]\nadata.var_names = [f\"Gene_{i:d}\" for i in range(adata.n_vars)]\nprint(adata.obs_names[:10])\n\nIndex(['Cell_0', 'Cell_1', 'Cell_2', 'Cell_3', 'Cell_4', 'Cell_5', 'Cell_6',\n       'Cell_7', 'Cell_8', 'Cell_9'],\n      dtype='object')\n\n\nAfter running the code above, the adata object created has a data matrix X attribute which essentially looks like the below:\n\n\n\nadata object\n\n\nThis can also be observed in python by running the below which converts the aData object into a dataframe using to_df.\n\n# outputs a dataframe version of the X matrix\nadata.to_df()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nGene_0\nGene_1\nGene_2\nGene_3\nGene_4\nGene_5\nGene_6\nGene_7\nGene_8\nGene_9\n...\nGene_1990\nGene_1991\nGene_1992\nGene_1993\nGene_1994\nGene_1995\nGene_1996\nGene_1997\nGene_1998\nGene_1999\n\n\n\n\nCell_0\n2.0\n0.0\n0.0\n1.0\n1.0\n1.0\n2.0\n0.0\n1.0\n2.0\n...\n0.0\n1.0\n1.0\n0.0\n1.0\n2.0\n1.0\n1.0\n1.0\n3.0\n\n\nCell_1\n0.0\n4.0\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n...\n2.0\n3.0\n0.0\n1.0\n1.0\n1.0\n3.0\n2.0\n1.0\n1.0\n\n\nCell_2\n0.0\n0.0\n2.0\n1.0\n0.0\n0.0\n1.0\n2.0\n0.0\n1.0\n...\n1.0\n2.0\n0.0\n1.0\n0.0\n2.0\n3.0\n1.0\n2.0\n1.0\n\n\nCell_3\n2.0\n1.0\n1.0\n2.0\n1.0\n0.0\n1.0\n2.0\n1.0\n0.0\n...\n2.0\n0.0\n2.0\n1.0\n1.0\n1.0\n1.0\n0.0\n3.0\n0.0\n\n\nCell_4\n1.0\n2.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nCell_95\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n2.0\n0.0\n0.0\n1.0\n...\n0.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.0\n1.0\n0.0\n3.0\n\n\nCell_96\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n1.0\n1.0\n2.0\n...\n1.0\n0.0\n1.0\n1.0\n0.0\n2.0\n0.0\n1.0\n1.0\n0.0\n\n\nCell_97\n1.0\n0.0\n4.0\n2.0\n1.0\n0.0\n2.0\n0.0\n1.0\n1.0\n...\n3.0\n3.0\n1.0\n0.0\n1.0\n2.0\n0.0\n0.0\n0.0\n0.0\n\n\nCell_98\n3.0\n0.0\n0.0\n1.0\n2.0\n0.0\n1.0\n0.0\n2.0\n1.0\n...\n0.0\n1.0\n0.0\n4.0\n1.0\n0.0\n1.0\n0.0\n3.0\n2.0\n\n\nCell_99\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n3.0\n2.0\n...\n0.0\n0.0\n1.0\n2.0\n0.0\n0.0\n2.0\n0.0\n0.0\n2.0\n\n\n\n\n100 rows √ó 2000 columns\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\nNow, let‚Äôs add in some annotations/metadata at the observation level. This could be the cell type of each observation.\n\n# random cell assignment\nct = np.random.choice([\"B\", \"T\", \"Monocyte\"], size=(adata.n_obs,))\nadata.obs[\"cell_type\"] = pd.Categorical(ct)  # Categoricals are preferred for efficiency\nadata.obs\n\n\n\n  \n    \n      \n\n\n\n\n\n\ncell_type\n\n\n\n\nCell_0\nB\n\n\nCell_1\nB\n\n\nCell_2\nB\n\n\nCell_3\nMonocyte\n\n\nCell_4\nMonocyte\n\n\n...\n...\n\n\nCell_95\nB\n\n\nCell_96\nB\n\n\nCell_97\nB\n\n\nCell_98\nT\n\n\nCell_99\nB\n\n\n\n\n100 rows √ó 1 columns\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\nFor metadata that has many dimensions (each cell could have a 2-dim UMAP mapping or each gene could have a 5-dim feature set), we can use the obsm and varm attributes as shown below.\n\n# add n-dim metadata to variables\nadata.obsm[\"X_umap\"] = np.random.normal(0, 1, size=(adata.n_obs, 2))\nadata.varm[\"gene_stuff\"] = np.random.normal(0, 1, size=(adata.n_vars, 5))\n\nThe above is a very brief overview of the anndata object. For more information, see Getting started with anndata.\nLet‚Äôs move on to some of the functions of scanpy.\n # scanpy demo\nFirst, let‚Äôs download and read in the demo 3k PBMC scRNA-seq data from 10X Genomics (company that provides services single-cell data generation and analysis).\n\n# fetch the scanpy demo data\n!mkdir data\n!wget http://cf.10xgenomics.com/samples/cell-exp/1.1.0/pbmc3k/pbmc3k_filtered_gene_bc_matrices.tar.gz -O data/pbmc3k_filtered_gene_bc_matrices.tar.gz\n!cd data; tar -xzf pbmc3k_filtered_gene_bc_matrices.tar.gz\n\n# read in the data\n# place cursor after first parantheses and push ctrl/cmd+shift+space bar to bring up docstrings\nadata = sc.read_10x_mtx(\n    'data/filtered_gene_bc_matrices/hg19/',  # the directory with the `.mtx` file\n    var_names='gene_symbols',                # use gene symbols for the variable names (variables-axis index)\n    cache=True)                              # write a cache file for faster subsequent reading\n\n--2023-07-31 16:15:40--  http://cf.10xgenomics.com/samples/cell-exp/1.1.0/pbmc3k/pbmc3k_filtered_gene_bc_matrices.tar.gz\nResolving cf.10xgenomics.com (cf.10xgenomics.com)... 104.18.0.173, 104.18.1.173, 2606:4700::6812:1ad, ...\nConnecting to cf.10xgenomics.com (cf.10xgenomics.com)|104.18.0.173|:80... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://cf.10xgenomics.com/samples/cell-exp/1.1.0/pbmc3k/pbmc3k_filtered_gene_bc_matrices.tar.gz [following]\n--2023-07-31 16:15:41--  https://cf.10xgenomics.com/samples/cell-exp/1.1.0/pbmc3k/pbmc3k_filtered_gene_bc_matrices.tar.gz\nConnecting to cf.10xgenomics.com (cf.10xgenomics.com)|104.18.0.173|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 7621991 (7.3M) [application/x-tar]\nSaving to: ‚Äòdata/pbmc3k_filtered_gene_bc_matrices.tar.gz‚Äô\n\ndata/pbmc3k_filtere 100%[===================&gt;]   7.27M  --.-KB/s    in 0.07s   \n\n2023-07-31 16:15:41 (97.9 MB/s) - ‚Äòdata/pbmc3k_filtered_gene_bc_matrices.tar.gz‚Äô saved [7621991/7621991]",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>[scanpy](https://scanpy.readthedocs.io/en/stable/)</span>"
    ]
  },
  {
    "objectID": "Lesson_8c_scanpy/Lesson_8c_scanpy.html#scanpy---explore-and-filter-data",
    "href": "Lesson_8c_scanpy/Lesson_8c_scanpy.html#scanpy---explore-and-filter-data",
    "title": "scanpy",
    "section": "scanpy - explore and filter data",
    "text": "scanpy - explore and filter data\nLet‚Äôs first look at the most highly expressed 20 genes in our dataset:\n\nsc.pl.highest_expr_genes(adata, n_top=20, )\n# return the number of observations\nprint(adata.n_obs)\n# return the number of variables\nprint(adata.n_vars)\n\n\n\n\n\n\n\n\n2700\n32738\n\n\nNow, let‚Äôs do some filtering for gene and cell representation.\n\nsc.pp.filter_cells(adata, min_genes=200)\nsc.pp.filter_genes(adata, min_cells=3)\n# return the number of observations\nprint(adata.n_obs)\n# return the number of variables\nprint(adata.n_vars)\n# looks like no cells were removed and 19,024 genes were removed\n# add some zeros if expression below a certain level\n# adata.X[adata.X &lt; 0.3] = 0\n\n2700\n13714\n\n\nFor brevity, the steps below filter out cells of poor quality (containing high proportions of mitochondrial genes) and also perform some normalization. See Preprocessing and clustering 3k PBMCs for more details. In the next section we will look at principal components and do some clustering to see if we can group cells with similar expression profiles.\n\n# filter out poor quality cells\nadata.var['mt'] = adata.var_names.str.startswith('MT-')  # annotate the group of mitochondrial genes as 'mt'\nsc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True)\nadata = adata[adata.obs.n_genes_by_counts &lt; 2500, :]\nadata = adata[adata.obs.pct_counts_mt &lt; 5, :]\n\n# normalization\nsc.pp.normalize_total(adata, target_sum=1e4)\nsc.pp.normalize_total(adata, target_sum=1e4)\nsc.pp.log1p(adata)\nsc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5)\nadata.raw = adata\n\n# filter out highly variable genes\nadata = adata[:, adata.var.highly_variable]\nsc.pp.regress_out(adata, ['total_counts', 'pct_counts_mt'])\nsc.pp.scale(adata, max_value=10)",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>[scanpy](https://scanpy.readthedocs.io/en/stable/)</span>"
    ]
  },
  {
    "objectID": "Lesson_8c_scanpy/Lesson_8c_scanpy.html#scanpy---pca-and-umap-clustering",
    "href": "Lesson_8c_scanpy/Lesson_8c_scanpy.html#scanpy---pca-and-umap-clustering",
    "title": "scanpy",
    "section": "scanpy - PCA and UMAP clustering",
    "text": "scanpy - PCA and UMAP clustering\nPCA stands for principal component analysis. principal components (PCs) are axes capturing variation in your data. They are often used to reduce the dimensionality of your dataset and can be used in machine learning/regression models. See A Step-By-Step Introduction to PCA for a more detailed overview. Let‚Äôs calculate the PCs and visualize the first two PCs highlighting CST3 expression.\n\n# look at pcs to see how many pcs to use in neighborhood graph construction\nsc.tl.pca(adata, svd_solver='arpack')\n# pl ie plot just the first two principal components\nsc.pl.pca(adata, color='CST3')\n\n\n\n\n\n\n\n\nIn the figure above, each dot is a cell plotted against the first two PCs. The color of the dot is correlated with CST3 expression. It looks like there are three or four different clusters just based on these PCs and CST3 expression level.\nNow, let‚Äôs create an elbow plot which will plot variance captured vs each PC. This gives us an idea of which PCs to use in clustering (those that capture the most variance).\n\n# note that this is a logarithmic scale of variance ratio\nsc.pl.pca_variance_ratio(adata, log=True)\n\n\n\n\n\n\n\n\nIn order to perform clustering, we need to compute the neighborhood graph using and embed the graph in UMAP (Uniform Manifold Approximation and Projection) dimensions. Neighborhood graphs are first determined where nodes represent cells and lines indicate degrees of similarity between cells ie lines with greater weight indicate cells are more closely similar to each other.\n\n\n\ngraph example\n\n\nKnowing this, we then embed the graph in UMAP dimensions. UMAP is another dimension reduction technique but is based on the idea that most high dimensional data lies in manifolds. We won‚Äôt go into much detail here regarding UMAP, but the below links are helpful to learn more:\n\nHow UMAP Works\nDimensionality Reduction for Data Visualization: PCA vs TSNE vs UMAP vs LDA\nMcInnes et al.¬†(2018)\n\n\n# calculate neighborhood graph pp = preprocessing using the first 40 PCs\nsc.pp.neighbors(adata, n_neighbors=10, n_pcs=40)\n\n# initial clustering...this part isn't in the official demo but I think they forgot this part\nsc.tl.leiden(adata)\n# remedy disconnected clusters...\nsc.tl.paga(adata) # maps \"coarse-grained connectivity structures of complex manifolds\", tl = toolkit, paga = partition-based graph abstraction\nsc.pl.paga(adata, plot=False)  # compute the course grained layout, pl = plot\nsc.tl.umap(adata, init_pos='paga') # embed in umap\n\n# embedding of neighborhood graph using UMAP\nsc.tl.umap(adata)\nsc.pl.umap(adata, color=['CST3', 'NKG7', 'PPBP'])\n\n\n\n\n\n\n\n\nNow, we can finally cluster the data using the Leiden graph-clustering method, which tries to detect communities of nodes.\nAgain, we won‚Äôt go into too much detail regarding this methods, but the below are helpful:\n\nCommunity Detection Algorithms\nTraag et al.¬†(2018)\n\n\nsc.tl.leiden(adata)\nsc.pl.umap(adata, color=['leiden', 'CST3', 'NKG7'])",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>[scanpy](https://scanpy.readthedocs.io/en/stable/)</span>"
    ]
  },
  {
    "objectID": "Lesson_8c_scanpy/Lesson_8c_scanpy.html#in-class-exercises",
    "href": "Lesson_8c_scanpy/Lesson_8c_scanpy.html#in-class-exercises",
    "title": "scanpy",
    "section": "In-class exercises",
    "text": "In-class exercises\nIn-class exercise 1: From the AnnData section‚Ä¶instead of creating a csr_matrix can we create a pandas dataframe instead to look at the data more easily?\nAnswer:\n\npd.DataFrame(np.random.poisson(1, size=(100, 2000)))\n\nIn-class exercise 2: Find the UMAP mappings for cell 5 in the adata object.\nAnswer:\n\nadata.obsm[\"X_umap\"][4]",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>[scanpy](https://scanpy.readthedocs.io/en/stable/)</span>"
    ]
  }
]